{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a843c1d",
   "metadata": {},
   "source": [
    "# 1. 패딩(Padding)\n",
    "- 자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있음\n",
    "- 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요\n",
    "\n",
    "- 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라 함.\n",
    "- 숫자 0을 사용하고 있다면 제로 패딩(zero padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83c0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==모든 단어들을 각 정수로 매핑 후 출력==\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "\n",
      "==가장 길이가 긴 문장의 길이==\n",
      "7\n",
      "\n",
      "==문장의 길이를 7로 맞춰줌==\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 문장\n",
    "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
    "\n",
    "# 텍스트 시퀀스의 모든 단어들을 각 정수로 맵핑 후 출력\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print('\\n==모든 단어들을 각 정수로 매핑 후 출력==')\n",
    "print(encoded)\n",
    "\n",
    "# 모두 동일한 길이로 맞춰주기 위해서 이 중에서 가장 길이가 긴 문장의 길이를 계산\n",
    "max_len = max(len(item) for item in encoded)\n",
    "print('\\n==가장 길이가 긴 문장의 길이==')\n",
    "print(max_len)\n",
    "\n",
    "# 문장의 길이를 7로 맞춰줌\n",
    "for item in encoded: # 각 문장에 대해서\n",
    "    while len(item) < max_len:   # max_len보다 작으면\n",
    "        item.append(0)\n",
    "\n",
    "padded_np = np.array(encoded)\n",
    "print('\\n==문장의 길이를 7로 맞춰줌==')\n",
    "print(padded_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6ad1e",
   "metadata": {},
   "source": [
    "# 2. 케라스 전처리 도구로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879cd570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==모든 단어들을 각 정수로 매핑 후 출력==\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "\n",
      "==keras의 pad_sequence를 사용한 패딩==\n",
      "[[ 0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  1  8  5]\n",
      " [ 0  0  0  0  1  3  5]\n",
      " [ 0  0  0  0  0  9  2]\n",
      " [ 0  0  0  2  4  3  2]\n",
      " [ 0  0  0  0  0  3  2]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  2]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 0  0  0  1 12  3 13]]\n",
      "\n",
      "==keras의 pad_sequence + post 옵션을 사용한 패딩==\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n",
      "\n",
      "==keras의 pad_sequence + 길이5 + post 옵션을 사용한 패딩==\n",
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n",
      "\n",
      "==keras의 pad_sequence + 길이5 + post 옵션 + 임의정수값 을 사용한 패딩==\n",
      "[[ 1  5 10 10 10 10 10]\n",
      " [ 1  8  5 10 10 10 10]\n",
      " [ 1  3  5 10 10 10 10]\n",
      " [ 9  2 10 10 10 10 10]\n",
      " [ 2  4  3  2 10 10 10]\n",
      " [ 3  2 10 10 10 10 10]\n",
      " [ 1  4  6 10 10 10 10]\n",
      " [ 1  4  6 10 10 10 10]\n",
      " [ 1  4  2 10 10 10 10]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encoding 한 값\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print('\\n==모든 단어들을 각 정수로 매핑 후 출력==')\n",
    "print(encoded)\n",
    "\n",
    "# 케라스의 pad_sequences를 사용하여 패딩\n",
    "padded = pad_sequences(encoded)\n",
    "\n",
    "# pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채우기 때문\n",
    "print('\\n==keras의 pad_sequence를 사용한 패딩==')\n",
    "print(padded)\n",
    "\n",
    "padded = pad_sequences(encoded, padding = 'post')\n",
    "print('\\n==keras의 pad_sequence + post 옵션을 사용한 패딩==')\n",
    "print(padded)\n",
    "\n",
    "print('\\n==keras의 pad_sequence + 길이5 + post 옵션을 사용한 패딩==')\n",
    "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)\n",
    "print(padded)\n",
    "\n",
    "print('\\n==keras의 pad_sequence + 길이5 + post 옵션 + 임의정수값 을 사용한 패딩==')\n",
    "padded = pad_sequences(encoded, padding = 'post', value = 10)\n",
    "print(padded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7b5cc",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding\n",
    "- 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다. 이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8c283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==Okt를 이용한 Token화==\n",
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "\n",
      "==Token 인덱스화==\n",
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n",
      "\n",
      "==One-hot-Encoding==\n",
      "[0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
    "print('\\n==Okt를 이용한 Token화==')\n",
    "print(token)\n",
    "\n",
    "word2index={}\n",
    "for voca in token:\n",
    "     if voca not in word2index.keys():\n",
    "        word2index[voca]=len(word2index)\n",
    "        \n",
    "print('\\n==Token 인덱스화==')\n",
    "print(word2index)\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "    index=word2index[word]\n",
    "    one_hot_vector[index]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "print('\\n==One-hot-Encoding==')\n",
    "print(one_hot_encoding(\"자연어\",word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e4eb7",
   "metadata": {},
   "source": [
    "## 4. 케라스를 이용한 원-핫 인코딩\n",
    "- to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba21f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==Token 인덱스화==\n",
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "\n",
      "==정수 Sequence로 texts_to_sequences==\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "\n",
      "==One-hot-encoding-keras==\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print('\\n==Token 인덱스화==')\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력.\n",
    "\n",
    "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded=t.texts_to_sequences([sub_text])[0]\n",
    "print('\\n==정수 Sequence로 texts_to_sequences==')\n",
    "print(encoded)\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "print('\\n==One-hot-encoding-keras==')\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f26d4",
   "metadata": {},
   "source": [
    "## 5. 원-핫 인코딩(One-Hot Encoding)의 한계\n",
    "- 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점\n",
    "- 저장 공간 측면에서는 매우 비효율적인 표현 방법\n",
    "- 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지\n",
    "    - 첫째는 카운트 기반의 벡터화 방법인 LSA, HAL \n",
    "    - 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
