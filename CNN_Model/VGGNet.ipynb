{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "- 2014년 준우승\n",
    "- Very deep convolutional networks for large-scale image recognition 으로\n",
    "- 원래는 망의 깊이가 어떤 영향을 주는지 연구를 하기위해 개발\n",
    "- **receptive field의 크기를 3x3으로 고정하고 깊이가 어떤 영향을 주는지 6개의 구조에 대해 실험**\n",
    "- 11layer(A), 13layer(B), 16layer(D), 19layer(E) 에 대해서 실험\n",
    "![vggnet_table](img/vggnet_table.png)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### VGGNet 구조적 특징\n",
    "- **모든 층에서 3x3 필터 사용** \n",
    "- 5x5 => 3x3 필터 2번 적용효과 / 7x7 => 3x3 필터 3번 적용효과\n",
    "- 크기가 2인 Maxpooling\n",
    "- Fully Connected layer에서 파라미터의 갯수가 엄청 많음\n",
    "- 1x1이 적용되지만 적용 목적 : 차원은 그대로 유지하면서 ReLU를 이용한 추가적인 non-linearity를 확보하기 위함\n",
    "- GoogleNet이나 NIN의 1x1 경우는 차원을 줄이기 위한 목적   \n",
    "    \n",
    "<hr>\n",
    "\n",
    "### VGGNet 학습 특징\n",
    "- Deep net은 학습할 떄 vanishing / exploding gradient 문제로 학습이 어려워질 수 있는데, 이것을 먼저 11 layer의 비교적 간단한 구조 - A 를 학습시킨 후 더 깊은 나머지 구조 학습시에는 처음 4 layer와 마지막 fully-connected layrer의 경우 구조-A의 학습결과로 초기값 설정한 후 학습\n",
    "- **11-layer학습 후 해당 가중치를 초기값으로 이용했다는 것**\n",
    "- 해당 내용은 GoogleNet의 auxiliary classifer와 유사\n",
    "\n",
    "<hr>\n",
    "\n",
    "### VGGNet 실험 결과\n",
    "- Deep하면 Deep할수록 좋은 성능을 얻었다.\n",
    "    \n",
    "![vggnet16_19](img/vggnet16_19.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
