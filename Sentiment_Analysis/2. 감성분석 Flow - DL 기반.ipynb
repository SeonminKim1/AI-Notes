{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8976418",
   "metadata": {},
   "source": [
    "## 2.1 Corpus 수집 및 정제\n",
    "- 블로그, 신문기사, 위키피디아, 기업 공시 정보, 특허 정보, 소설 등 다양한 출처로부터 말뭉치(Corpus) 수집\n",
    "- 한국어 위키 말뭉치\n",
    "    - https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4_%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C\n",
    "    - pages-articles.xml.bz2 파일을 다운로드\n",
    "    - wikiextractor를 이용해 처리된 결과 파일을 텍스트로 변환화는 과정을 거쳐야 함\n",
    "    - 위 과정을 하나의 프로그램으로 만들어 놓은 web-crawler\n",
    "- 정제\n",
    "    - 문장단위 분리 및 제목, 리스트 , 목차 등 .. 제거\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8b786",
   "metadata": {},
   "source": [
    "## 2.2 Vocab 개념 및 기본전처리\n",
    "- vocab은 vocabulary의 약자로 단어들의 사전\n",
    "- BERT 모델에서 Corpus를 읽을 때 Vocab을 기준으로 읽음\n",
    "- [PAD], [UNK], [CLS], [SEP]. [MASK], 런던, 상황이다, ##훈련, ##기구\n",
    "    - CLS : 시작, SEP : 끝남, \n",
    "- 부가적인 전처리\n",
    "    - 한국어 분석에 불필요한 표제어는 필터링\n",
    "    - [UNK]의 숫자를 최소화하기 위해서 검수 작업도 필수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3031ca5",
   "metadata": {},
   "source": [
    "## 2.3 Tokenizing (형태소 분절)\n",
    "- 입력으로 들어온 문장들에 대해 토큰으로 나누어 주는 역할\n",
    "- Word Tokenizer와 Subword Tokenizer으로 나뉜\n",
    "    - Character level\n",
    "        - vocab을 만드는 방법 (ㄱ,ㅓ, ㅁ,ㅏ) 성능 별로 안좋음\n",
    "    - Space level\n",
    "        - 조사/어미 등의 경우 중복단어 문제발생('책이','책을','책에')\n",
    "    - Word Tokenizer\n",
    "        - 단어 기준으로 토큰화\n",
    "    - Subword tokenizer\n",
    "        - 단어 내 단어들로 토큰화\n",
    "        - 단어의 빈도수를 계산해서 subword 단위로 쪼개는 방법( ex- BPE)\n",
    "        - vocab에 없는 단어들에 대해서도 좋은 성능을 보인다는 장점을 가짐\n",
    "\n",
    " \n",
    "### 2.3.1 Tokenizing - Subword, BPE 기반 \n",
    "- Subword를 이용하면 적은 수의 vocabulary를 가지고 OOV(Out of Vocabulary)를 최소화\n",
    "- Subword의 분할이 의미 기준이 아닐 수 있음 ('수원에' -> '수, '원에')로 될 수 있음\n",
    "- \n",
    "- **Huggingface tokenizer : Wordpiece**\n",
    "    - Huggingface의 Tokenizer는 4가지 분절 학습 모델을 제공\n",
    "    - subword tokenizer의 종류 중 하나 (BPE(Byte Pair Encoding) 방법)\n",
    "    - likelihood를 기반으로 BPE를 수행한 알고리즘\n",
    "    - BPE는 말뭉치내 가장 많이 등장한 문자열등을 병합해 문자열을 압축\n",
    "        - 경찰차, 경찰관, 경찰복 => 경찰, ##차, ##관, ##복\n",
    "    - 예측에서의 BPE\n",
    "        - 1) 문장 내 각 어절(띄어쓰기)에 vocab에 있는 서브워드(Subword)가 포함되어 있을 경우 해당 서브워드를 어절에서 분리\n",
    "        - 2) 어절 내에서 vocab에 있는 서브워드를 다시 찾고 또 분리\n",
    "        - 3) 최종 탐색 후 어휘 집합에 없으면 미등록단어(unk)로 취급\n",
    "    - BERT의 경우 Wordpiece를 이용한 tokenizer를 사용\n",
    "\n",
    "- **Google : Sentencepiece**\n",
    "    - google에서 제공하는 Tokenizer tool\n",
    "    - 빈도수를 기반으로 BPE를 수행 (=기본 unigram)\n",
    "    - \n",
    "- Mecab\n",
    "    - mecab 형태소 분석기를 이용해 분절\n",
    "    \n",
    "### 2.3.2 Tokenizer 전후\n",
    "- 테스트 문장:   [CLS] 나는 워드피스 토크나이저를 써요. 성능이 좋은지 테스트 해보려 합니다. [SEP]\n",
    "- 문장 인코딩:  [2, 9310, 4868, 6071, 12467, 21732, 12200, 6126, 6014, 4689, 6100, 18, 11612, 6037, 9389, 6073, 16784, 17316, 6070, 10316, 18, 3]\n",
    "- 문장 디코딩:  [CLS] 나는 워드피스 토크나이저를 써요. 성능이 좋은지 테스트 해보려 합니다. [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b963f0",
   "metadata": {},
   "source": [
    "## 2.4 Embedding\n",
    "- 자연어를 토큰화 및 인덱스를 부여해, 원핫 벡터나 학습을 통해 밀집된 벡터들을 만드는 방법\n",
    "- 밀집된 벡터를 만들기 위해 단어를 벡터로 표현하는 것을 워드임베딩(word embedding)이라고 함\n",
    "- 단어 수준 임베딩 기법 \n",
    "    - NPLM, Word2Vec, FASTtEXT, 잠재의미분석(LSA), Glove, Swivel \n",
    "- 문장 수준 임베딩기법\n",
    "    - LSA, Dov2Vec, 잠재디레클레할당(LDA), ELMo, BERT\n",
    "- 임베딩에는 말뭉치의 의미적, 문법적 맥락이 포함되어있음\n",
    "- 소규모 데이터를 이용하여, 임베딩을 포함한 모델 전체를 업데이트 진행\n",
    "- 이와 같이 자연어 처리의 구체적 문제들을 Downstream Task 라고 지칭함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b9727",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://paul-hyun.github.io/vocab-with-sentencepiece/ // 한국어 위키 말뭉치 만들기\n",
    "- https://twoblockai.com/%ED%95%9C%EA%B5%AD%EC%96%B4-bert-%EA%B5%BD%EA%B8%B0/ // 투블럭 AI HanBert\n",
    "- https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0 // 나만의 WordPiece 만들기\n",
    "\n",
    "- https://keep-steady.tistory.com/37 : 서브워드 구축 (mecab, huggingface, sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8976abf",
   "metadata": {},
   "source": [
    "## 해볼일\n",
    "\n",
    "- https://paul-hyun.github.io/nlp-tutorial-02-02-tokenizer/ : 한국어위키로 vocab 만들기 + 자체 데이터 추가해서 vocab 만들기\n",
    "\n",
    "## 전체 Flow 정리 \n",
    "- 결국엔 vocab을 이용한 기본 단어사전 구축 = Tokenizing 이 활용됨\n",
    "- Tokenizing tool을 통해 정수인코딩을 하고 Embedding layer에 집어넣게 됨.\n",
    "- embedding layer를 통해 fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
