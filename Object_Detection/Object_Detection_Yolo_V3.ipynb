{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO V3\n",
    "- paper : An Incrementatl Improvement (2018)\n",
    "- **Tech Report 느낌** 의 다른 Detection Algorithm 들의 방법들을 가져와 도움을 얻은 것들 및 다양한 경우 소개\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bounding Box Prediction\n",
    "\n",
    "![yolo_v3_boundingbox](img/yolo_v3_boundingbox.jpg)\n",
    "\n",
    "### Bounding Box Prediction과 관련된 파라미터\n",
    "- 쉽게 말해서 c_x, c_y가 존재(현재 위치느낌), yolo가 클러스터링을 통해서 anchor box를 생성함.\n",
    "- 1) c_x, c_y : grid 셀의 좌상단의로부터의 좌표(offset)\n",
    "    - YOLO는 이미지를 S*S grid 셀로 나눔\n",
    "    - 예를 들어 7*7 grid 셀로 설정하면, 한 이미지에 7*7 = 49개의 셀이 생성\n",
    "\n",
    "- 2) p_w, p_h\n",
    "    - 앵커박스의 너비 및 높이\n",
    "- 3) t_x, t_y, t_w, t_h\n",
    "    - YOLO가 예측해야하는 물체의 좌표 값 (bounding box)\n",
    "    - 시그모이드 함수를 통한 t_x, t_y를 0~1 사이의 값으로 초기화\n",
    "- 4) b_x, b_y, b_w, b_h\n",
    "    - 좌상단 offset으로 부터 얼마나 이동할것이고 (b_x, b_y)\n",
    "    - 앵커박스의 너비와 높이를 얼만큼 비율로 조절할지 (b_w, b_h) 지수승을 통해 예측\n",
    "    - 위 파라미터값을 조정한 최종 bounding box의 offsets 값들\n",
    "    - 이를 토대로 GT(Ground Truth, 라벨 값)와 IOU를 계산\n",
    "\n",
    "- 각각의 예측된 box마다 Objectness에 대한 점수를 logistic regression을 통해 구함\n",
    "    - 즉 만약 예측된 어떤 box가 다른 box들보다 IOU가 높다면 해당값은 1\n",
    "    - 만약 IOU가 제일 좋지는 않지만 특정한 Threshold보다 높다면 Faster R-CNN 방법을 사용해 무시\n",
    "- YOLO v3는 IOU Threshold값을 0.5를 사용\n",
    "- 하나의 box에 하나의 GT Box를 할당, box가 GT에 할당되지 않으면 좌표 및 클래스 예측에 손실을 주지 않고, Objectness에 대한 손실값만 적용\n",
    "\n",
    "### Faster R-CNN Anchor Box vs Yolo v3 Anchor Box\n",
    "- Faster R-CNN\n",
    "    - 특정 Anchor가 Ground Truth와의 IOU가 0.7보다 크면 -> 해당 anchor box를 positive\n",
    "    - 특정 Anchor가 Ground Truth와의 IOU가 0.3보다 작으면 -> 해당 anchor box는 non-positive\n",
    "    - 나머지 구간 -> IOU가 가장 큰 anchor box를 positive로\n",
    "    \n",
    "- Yolo V3 \n",
    "    - 특정 Anchor가 Ground Truth와의 IOU가 0.5보다 크면 -> 해당 Anchor Box를 Positive\n",
    "    - 특정 Anchor가 Ground Truth와의 IOU가 0.5보다 작으면 -> 해당 경우 무시 및 Objectness에 대한 손실값 적용\n",
    "\n",
    "  - b_x, b_y : 시그모이드 함수(sigmoid function)를 통해 t_x, t_y를 0~1 사이의 값으로 초기화\n",
    "  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Prediction\n",
    "- 각각의 box는 클래스 예측시 Multi-label classification을 하게 됨\n",
    "- 기존에는 class prediction시 softmax를 사용\n",
    "    - but. softmax를 사용하는 것이 성능면에서 좋지 않다고 함.\n",
    "    - yolo v3에서는 Softmax가 아닌 logistic classifiers가 좋다고 함\n",
    "    - loss = binary cross-entropy 사용\n",
    "\n",
    "- 문제점 및 접근 방법\n",
    "    - 라벨들의 mutually inclusive 한 데이터셋 경우 더 적합\n",
    "        - ex) Google의 Open Image Dataset 경우 label들(사람 & 여자)등이 겹치게 됨.\n",
    "        - 이런 애매한 경우 model을 multilabel방식으로 접근하는 것이 더 좋은 접근방법\n",
    "    - 라벨들이 mutually exclusive 한 경우 softmax가 좋음\n",
    "    \n",
    "<hr>\n",
    "\n",
    "## 3. Predictions Across Scales\n",
    "- 3종류의 다른 스케일의 box들을 예측 \n",
    "- feature pyramid network의 컨셉과 유사한 방식으로 다양한 스케일에서 feature를 추출\n",
    "- 출력값 : 3-d tensor => box정보, objectness, class 정보\n",
    "    - N x N x [3*(4+1+80)] => (N, N, 255)\n",
    "    - N x N의 Grid cell 갯수, anchor box의 갯수 3개, 4개의 box변수(box shift and scailing value), 1개의 objectness score, 80개의 classes\n",
    "        \n",
    "- (1) 이전 2번째 layer에서 feature map 가지고 와서 2배로 upscaling\n",
    "- (2) 제일 앞단의 feature map을 가져와 둘을 concatenation하여 upsample(늘어난)된 feature map과 합침.\n",
    "- (3) fine-grained information을 제일 앞단의 feature map으로 부터 얻음.\n",
    "- (4) 최종적으로 3가지 multi-scale의 fine-grained feature를 이용한 box 예측\n",
    "\n",
    "- yolo v3 에서도 기존 k-means를 이용한 anchor box dimension clustering 그대로 유지\n",
    "    - yolo v3에서는 9개의 clsuter를 사용 및 3개의 scale에 대해 임의로 anchor box dimension 할당\n",
    "    - COCO 기준\n",
    "        - (10x13, 16x30, 33x23, 30x61, 62x45, 59x119, 116x90, 156x198, 373x326)\n",
    "        \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extractor\n",
    "- YOLO V3 에서는 Feature Extraction을 위한 새로운 network 모델(Darknet-53) 사용\n",
    "- **YOLO V2, Darknet-19, Newfangled residual netwrok를 적절하게 섞어 사용.**\n",
    "- 3x3과 1x1 conv 레이어를 성공적으로 적용, shortcut connection으로 인해 네트워크 자체가 많이 커짐\n",
    "- 새로운 Layer는 Darknet-19모델보다 강력하며, resNet-101, 152 보다 조금더 효율적\n",
    "- \n",
    "\n",
    "![yolov3_darknet-53](img/yolov3_darknet-53.jpg)\n",
    "\n",
    "## Training\n",
    "- full image 사용\n",
    "- no hard negative mining\n",
    "    - IOU 낮은값들: Best IOU 값 비율 조정 안해주고 그냥 학습\n",
    "- multi-scale training\n",
    "- data augmentation\n",
    "- batch normalization\n",
    "\n",
    "## 성능\n",
    "- SSD와 성능은 비슷 하나 3배 더 빠름, RetinaNet보다는 성능이 조금 떨어짐\n",
    "\n",
    "![yolov3_inference_time](img/yolov3_inference_time.png)\n",
    "\n",
    "\n",
    "![yolo_v3_metrics](img/yolo_v3_metrics.jpg)\n",
    "\n",
    "- 기존의 Detection Metric인 AP50을 사용시 RetinaNet과 비슷해지며, SSD 변형보다 강력해짐\n",
    "- 과거 YOLO V1 이 작은 물체를 찾는데 좋았다면, YOLO V3은 작은 물체에는 조금약함\n",
    "- 하지만 Accuracy와 speed를 같이 본다면 YOLO V3는 꽤나 이점이 있다.\n",
    "\n",
    "\n",
    "## 추가실험내용\n",
    "- 추가 실험을 진행 하였지만 좋은 결과를 내진 못했다.\n",
    "- Anchor Box x,y offset predictions\n",
    "    - linear activation을 사용한 다양한 box의 width, height의 x,y offset을 일반적인 anchor box예측 메커니즘 사용하려 했으나 오히려 모델의 안정성을 해쳤음\n",
    "- Linear x,y predictions instead of logistic\n",
    "    - logistic activation 대신 linear activation을 사용하여 다이렉트로 x,y의 offset값을 예측하려고 했지만 실패\n",
    "    \n",
    "- Focal loss\n",
    "    - RetinaNet의 focal loss를 사용하였지만 잘 안되었다.\n",
    "\n",
    "### YOLO Loss vs Retina loss\n",
    "- RetinaNet Loss (Focal loss)\n",
    "    - Detection 시스템에서 background라는 클래스를 따로 학습시킴\n",
    "    - 이렇게 클래스를 background로 두게 되면, 한장의 이미지에서 Object가 있는 영역보다 없는 영역이 커짐\n",
    "    - 이는 굉장히 큰 class unbalance유발\n",
    "    - 그럴 경우 실제 Object가 있는 경우를 학습시키기 어려운 hard sample, Background 클래스의 경우는 학습시키기 쉬운 easy sample이다라고 생각\n",
    "    - 기존에 Class Loss로 사용하던 Cross-Entropy를 hard sample과 easy sample에 따라 loss를 다르게 줌\n",
    "    \n",
    "- Yolo Loss\n",
    "    - YOLO Loss는 Background, Object경우에 대한 Loss를 분할해서 주는 것이 아닌, Objectness score라는 부분을 따로 뽑고있음\n",
    "    - 저자 왈) YOLO에 Focal Loss를 주는거 자체가 큰 의미는 없을거라고 생각\n",
    "\n",
    "## 최종 결론\n",
    "- YOLO V3는 빠르고 정확하다는 관점에서의 좋은 Detector이다.\n",
    "- COCO Metric기준으로는 좋은편은 아니지만, 기존의 IOU 0.5기반 평가방법에서는 좋은 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://taeu.github.io/paper/deeplearning-paper-yolov3/\n",
    "- https://dhhwang89.tistory.com/138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
