{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "- 시계열 데이터 및 자연어 처리하는데 주로 사용\n",
    "- LSTM (Long short term memory), GRU(Gated recurrent unit)등이 언어 모델링(language modeling), 텍스트 감정 분석(text sentiment analysis, 기계 번역(machine translation) 등의 분야에서 활발하게 이용되고 있음\n",
    "- GRU(Gated recurrent unit)은 시계열 데이터 속 벡터 사이의 정보 전달량을 조절함으로서 기울기를 적정하게 유지하고 문장 앞부분의 정보가 끝까지 도달할 수 있도록 도와줌\n",
    "    - 업데이트 게이트(이전 은닉 벡터가 지닌 정보를 새로운 은닉 벡터가 얼마나 유지할지)와 리셋 게이트라는 개념이 존재\n",
    "    \n",
    "### RNN 종류\n",
    "- 1) one to one (일대일) : 일반적인 신경망, CNN과 동일\n",
    "- 2) one to many (일대다) : 이미지를 보고 이미지 안의 상황을 글로 설명하는 문제\n",
    "- 3) many to one (다대일) : 감정 분석 같이 순차적인 데이터를 보고 하나를 내는 경우\n",
    "- 4) many to many1 (다대다1) : 챗봇과 기계 번역 같이 순차적인 데이터를 보고 순차적인 데이터를 출력하는 문제\n",
    "- 5) many to many2 (다대다2) : 비디오 분류 같이 매 프레임을 레이블링 할 때 사용됨\n",
    "![rnn](readme_image/rnn.png)\n",
    "\n",
    "### 영화 리뷰 감정 분석\n",
    "- 다대일\n",
    "- IMDB 데이터\n",
    "    - 긍정은 2, 부정은 1\n",
    "    - 영화 리뷰 데이터 5만 건\n",
    "\n",
    "### 데이터 전처리\n",
    "- 텍스트를 전처리 과정을 거쳐 숫자로 나타내어야 함.\n",
    "\n",
    "- (1) 토크나이징 (Tokenization)\n",
    "    - 언어의 최소 단위인 토큰으로 나누는 것\n",
    "    - split() 함수 또는 Spacy 오픈 소스 라이브러리 사용 가능\n",
    "    - 데이터셋의 모든 단어 수만큼의 벡터를 담는 사전을 정의\n",
    "        - ex) quick brown fox jumps over the lazy dog => ['quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "- (2) 워드임베딩(word embedding)\n",
    "    - 문장 속 모든 토큰을 각각의 벡터로 나타내주는 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 라이브러리 import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets # 자연어 처리 데이터셋\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "batch_size =64\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# CUDA 여부 및 DEVICE 체크\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습셋]: 25000 [검증셋]: 5000 [테스트셋]: 25000 [단어수]: 46159 [클래스] 2\n"
     ]
    }
   ],
   "source": [
    "## 2. 데이터셋 확보\n",
    "\n",
    "# sequential 파라미터를 이용해 데이터셋이 순차적인 데이터셋인지 명시해줌\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first = True)\n",
    "\n",
    "# IMDB Train set, Test set 만들기\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL) \n",
    "\n",
    "# 워드 임베딩에 필요한 단어 사전 만들기 (word vocabulary)\n",
    "# min freq 미만 인 단어들은 Unknown이라는 뜻의 unk라는 토큰으로 대체됨.\n",
    "TEXT.build_vocab(trainset, min_freq = 5) #  min freq는 최소 몇번 등장한 단어마만을 사용하겠다는 뜻\n",
    "LABEL.build_vocab(trainset)\n",
    "\n",
    "# train, validation, test data 배치별로 뽑아주는 getnerator\n",
    "trianset, valset = trainset.split(split_ratio=0.8)\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (trainset, valset, testset), batch_size = batch_size, shuffle =True, repeat = False\n",
    ")\n",
    "\n",
    "# 사전 속 단어들의 갯수와 레이블 수 정해줌\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "\n",
    "print(\"[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스] %d\" %(len(trainset), len(valset), len(testset), vocab_size, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RNN 모델 구현\n",
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print('Building Basic GRU Model...')\n",
    "        self.n_layers = n_layers\n",
    "        ## embed 결과 -> quick:[0.1, 0.2], lazy[0.2, 0.8], dog[0.8, 0.9] \n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim) # 사전에 등재된 단어수, 임베딩된 단어 텐서가 지니는 차원 값\n",
    "        \n",
    "        # hyper parameter 값들\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # gru 모델 넣기. embed차원, hidden layer 차원, layer들\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers, batch_first=True)\n",
    "        \n",
    "        # 모델 최종 Dense 신경망\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes) # 입력된 텐서를 nn 신경망을 통과시켜 예측\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # embed 결과\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번쨰 은닉벡터\n",
    "        x, _ = self.gru(x, h_0) # 첫번째 은닉벡터인 h_0을 gru에 넣으면 은닉 벡터들이 시계열 배열 형태로 반환\n",
    "        \n",
    "        # (batch_size, 입력x의 길이, hidden_dim) 텐서들 => 영화데이터들이 압축된 은닉 벡터\n",
    "        h_t = x[:,-1,:]  \n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t) # self.out 신경망에 입력해 결과를 출력\n",
    "        return logit\n",
    "\n",
    "    # 신경망 모듈의 가중치 정보들을 반복자 형태로 반환\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data # nn.GRU 모듈의 첫번쨰 가중치 텐서를 추출\n",
    "        \n",
    "        # 그 후 NEW 함수를 호출해 모델의 가중치와 같은 모양인 텐서로 변환한후 zero()로 초기화\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() # zero_() : 모든값을 0으로 초기화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습함수\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    # 훈련 데이터 b=number , batch=영화평 데이터와, label들(긍부정)\n",
    "    for b, batch in enumerate(train_iter): \n",
    "        \n",
    "        # x는 영화평 데이터, y는 label들\n",
    "        # batch.text와 batch.label로 나뉨\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) # 레이블값을 0과 1로 변환\n",
    "        \n",
    "        # 함수로 기울기를 0으로 초기화\n",
    "        optimizer.zero_grad() \n",
    "        logit = model(x) # 학습 데이터x를 모델에 입력해 예측값(logit)을 구함\n",
    "        \n",
    "        # 오차 및 학습 진행\n",
    "        loss = F.cross_entropy(logit, y)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# 평가함수\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval() # evaluate mode 변경\n",
    "    corrects, total_Loss =0, 0 # 정답 수와, 총 loss\n",
    "\n",
    "    # test data\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) # 레이블값을 0과 1로 변환\n",
    "        logit = model(x)\n",
    "        \n",
    "        # loss 구하기, batch별 loss가 아닌 전체 데이터에 대한 loss 합 이기에 sum\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss +=loss.item()\n",
    "        \n",
    "        # 정답수\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data ==y.data).sum()\n",
    "    \n",
    "    # 총 loss, accuracy return\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0*corrects/size\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU Model...\n"
     ]
    }
   ],
   "source": [
    "## 모델 생성 및 학습 진행\n",
    "# 모델 생성 (은닉벡터의 차원값 : 256,  임베딩된 토큰의 차원값 : 128)\n",
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = None\n",
    "for e in range(1, epochs+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" %(e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 학습 오차가 가장 작은 것이 아닌 검증 오차가 젤 작은 것.\n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        # 디렉토리 없으면 만들기\n",
    "        if not os.path.isdir(\"model\"):\n",
    "            os.makedirs(\"model\")\n",
    "        \n",
    "        # 모델 저장 및 loss 저장\n",
    "        torch.save(model.state_dict(), 'model/sentiment_classification.pt')\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "# 저장된 모델 load 및 최종 테스트\n",
    "model.load_state_dict(torch.load('model/sentiment_classification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도 : %5.2f' %(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
