{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61eb0839",
   "metadata": {},
   "source": [
    "# 1. 불용어 제거 - 한국어 - 영어\n",
    "- 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요\n",
    "- NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262cea7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4631142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd9387",
   "metadata": {},
   "source": [
    "# 2. 불용어 제거 - 한국어\n",
    "- 토큰화 후에 조사, 접속사 등을 제거하는 방법이 일반적\n",
    "- 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생김\n",
    "- 결국 사용자가 직접 불용어 사전을 만들게 되는 경우가 많음\n",
    "- 한국어 불용어 리스트 : https://bab2min.tistory.com/544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7e8591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "# 직접 불용어를 정의, 주어진 문장으로부터 불용어 사전을 참고 불용어 제거\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba077702",
   "metadata": {},
   "source": [
    "# 3. 정수 인코딩\n",
    "- 방법 4가지) dictionary 사용하기 / Counter 사용하기 / NLTK의 FreqDist 사용하기 / enumerate 이해하기\n",
    "- 텍스트에 단어 5,000개의 단어들 각각에 1번부터 5,000번까지 단어와 맵핑되는 고유한 정수(인덱스)를 부여\n",
    "- 랜덤으로 부여 or 보통은 전처리 또는 빈도수가 높은 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ef5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===문장토큰화 출력===\n",
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n",
      "===단어토큰화 출력===\n",
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']] \n",
      "\n",
      "===단어사전 출력===\n",
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "8 \n",
      "\n",
      "===단어사전 빈도수 정렬===\n",
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고,\n",
    "# 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "\n",
    "# 문장 토큰화\n",
    "print('===문장토큰화 출력===')\n",
    "text = sent_tokenize(text)\n",
    "print(text)\n",
    "\n",
    "# 정제와 단어 토큰화\n",
    "vocab = {} # 파이썬의 dictionary 자료형\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "\n",
    "print('===단어토큰화 출력===')\n",
    "print(sentences, '\\n')\n",
    "\n",
    "print('===단어사전 출력===')\n",
    "print(vocab)\n",
    "print(vocab[\"barber\"],'\\n') # 'barber'라는 단어의 빈도수 출력\n",
    "\n",
    "print('===단어사전 빈도수 정렬===')\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7202bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===빈도수 상위 7개 단어 인덱스 출력===\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n",
      "\n",
      "===빈도수 상위 5개 단어 출력===\n",
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 빈도수 낮은 단어에 낮은 인덱스 부여\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "print('===빈도수 상위 7개 단어 인덱스 출력===')\n",
    "print(word_to_index)\n",
    "\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "\n",
    "print('\\n===빈도수 상위 5개 단어 출력===')\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6eb5d",
   "metadata": {},
   "source": [
    "## 3-1. OOV 및 추가\n",
    "- 단어 집합에 존재하지 않는 단어들을 Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "931e46db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
