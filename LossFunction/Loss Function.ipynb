{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 신경망 학습이란?\n",
    "- **현재의 상태를 '하나의 지표(손실함수)'로 표현, 해당 지표를 가장 좋게 만들어 주는 가중치 값을 탐색하는 것**\n",
    "\n",
    "\n",
    "\n",
    "## 1. 손실함수 (Loss Function)\n",
    "- Loss Function이란 목표한 파라미터의 최적화에 대한 목적함수\n",
    "- 일반적으로 '평균 제곱 오차(MSE)'와 '교차 엔트로피 오차(Cross Entropy)'를 사용, 모양이 아래로 볼록한(convex) 모양이 좋음\n",
    "- Input이 모델 함수를 지나 output을 예측, y_pred 와 실제값 y_true를 loss 함수를 통해 비교, 차이를 이용한 w update\n",
    "- **Loss Function (Convex 모양) ==> 경사하강법(Gradient Descent)에 의한 최솟값을 찾기가 쉽고(=최적화가 잘 된 다는 것) ==> 특정 가중치가 너무 큰 값을 가질 수 있음 (=학습데이터에 편중된 파라미터로 최적화될 가능성 존재) ==> Regularization penalty) 등장** \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1, L2 Loss\n",
    "- L1 Loss \n",
    "    - 실제 값과 예측치 사이의 차이(오차)의 절대값 및 그 오차들의 합\n",
    "    - 0인 지점에서 미분이 불가능\n",
    " \n",
    "![l1_loss](img/l1_loss.png)\n",
    "    \n",
    "    \n",
    "- L2 Loss\n",
    "    - 오차의 제곱 합\n",
    "    - 오차의 제곱이기 때문에 Outlier에 더 큰 영향을 받음\n",
    "    \n",
    "![l2_loss](img/l2_loss.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Loss Function\n",
    "\n",
    "### 3-1. Mean Absolute Error (MAE)\n",
    "- MAE도 Convex 모양\n",
    "![mae](img/mae.gif)\n",
    "\n",
    "### 3-2. Mean Squared Error (MSE)\n",
    "- 오차를 모두 제곱한 다음에 평균 낸 것\n",
    "![mse](img/mse.png)\n",
    "\n",
    "### 3-3. MAE vs MSE\n",
    "- MAE의 경우, loss가 크던 작던 항상 gradient가 일정, \n",
    "- Loss가 작아도 gradient는 작지 않기 때문에 경사하강법(gradient descent)를 사용해 최적값을 찾는 데에 어려움이 존재\n",
    "- MSE는 loss가 크면 gradient도 크고, loss가 작으면 gradient도 작기 때문에 최적값을 찾는 것이 보다 수월\n",
    "\n",
    "![mse_vs_mae](img/mse_vs_mae.png)\n",
    "\n",
    "\n",
    "### 3-4. Huber 손실 (후버손실)\n",
    "- MSE 와 MAE의 절충안\n",
    "- 일정한 범위(델타)안에 있으면 오차를 제곱, 밖에 있으면 오차의 절댓값 사용\n",
    "![huber](img/huber.png)\n",
    "\n",
    "![huber_img](img/huber_img.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Classification Loss Function\n",
    "### 4-1. Binary Cross Entropy \n",
    "- 이진 분류시 사용.\n",
    "- 손실함수는 예측값과 실제값이 같으면 0이 되어야 한다. \n",
    "    - 예측값과 실제값이 1로 같을 때 loss는 0\n",
    "    - 예측값이 0 실제값이 1이면 loss는 +Inf\n",
    "- torch.nn.BCELoss / torch.nn.BCEWithLogitsLoss(멀티 Label 용, 각각의 클래스에 대한 binary cross entropy)\n",
    "    - torch.nn.BCEWithLogitsLoss = torch.nn.BCEWithLoss + torch.sigmoid\n",
    "\n",
    "![binarycrossentropy](img/binarycrossentropy.PNG)\n",
    "\n",
    "\n",
    "### 4-2. Categorical cross-entropy\n",
    "- 멑티 클래스 분류 시 사용\n",
    "- **라벨이 [0,0,1], [1,0,0], [0,1,0] 처럼 one-hot 형태로 제공될 때 사용**\n",
    "- torch.nn.CrossEntropyLoss\n",
    "\n",
    "![categorical_crossentropy](img/categorical_crossentropy.PNG)\n",
    "\n",
    "### 4-3. Sparse categorical cross-entropy\n",
    "- 멀티 클래스 분류 시 사용\n",
    "- **라벨이 one-hot이 아닌 0,1,2,3,4 와 같이 정수의 형태로 제공될 때 사용**\n",
    "![sparse_vs_categorical](img/sparse_vs_categorical.png)\n",
    "\n",
    "### 4-4.\n",
    "- (4) 기타\n",
    "    - Hinge\n",
    "    - Squared Hinge\n",
    "    - Categorical Hinge\n",
    "    - nn.KLDivLoss\n",
    "    - nn.GaussianNLLLoss\n",
    "    - nn.SoftMarginLoss\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 참고문헌\n",
    "- https://www.youtube.com/watch?v=kuJROoa4kh8\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses // keras loss\n",
    "- https://pytorch.org/docs/stable/nn.html#loss-functions // pytorch loss\n",
    "- https://bo-10000.tistory.com/44\n",
    "- https://light-tree.tistory.com/125#:~:text=5.-,L1%20Loss,%EC%9D%84%20L1%20Loss%20%EB%9D%BC%EA%B3%A0%20%ED%95%A9%EB%8B%88%EB%8B%A4.\n",
    "- https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3\n",
    "- http://doc.mindscale.kr/km/data_mining/dm02.html\n",
    "- https://bskyvision.com/822"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
