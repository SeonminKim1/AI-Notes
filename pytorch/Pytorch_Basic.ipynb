{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 파이토치\n",
    "- 페이스북 주도로 여러 회사와 대학이 합심해 개발한 오픈소스 프로젝트\n",
    "- 2016년 8월 첫 공개\n",
    "\n",
    "### vs Numpy\n",
    "- 파이토치는 backward()라는 함수를 통해 그래프 연산 한번에\n",
    "- GPU 연산 가능 (GPU로 값을 보내 연산을 돌리고 다시 받는 것이 불가능)\n",
    "- 내부적인 CUDA, CUDNN API를 통해 GPU 연산에 사용 가능\n",
    "\n",
    "### vs Tensorflow\n",
    "#### 1. 텐서플로\n",
    "- 그래프를 미리 정해두고 실행중에는 바꿀 수 없음 (정적 계산 그래프 static computational graph) 방식\n",
    "- 정적 계산 그래프 방식은 그래프 계산 방식을 최초에 정해두기 때문에 최적화하기가 쉬움\n",
    "- 정적인 데이터셋 학습 및 정적인 모델 배포 -> 오프라인 학습(offline learning)\n",
    "\n",
    "#### 2. 파이토치\n",
    "- 동적 계산 그래프 방식 (dynamic computational graph)\n",
    "- 데이터에 대해 유연한 모델을 만들 수 있음\n",
    "- 자율주행차, 게임, 인터넷 등 쌓이는 데이터에 대한 실시간 대응하는 동적인 학습 방법이 중요해질 전망\n",
    "\n",
    "### 파이토치 라이브러리\n",
    "- Torchvision : 각종 비전 및 데이터 관리용 도구\n",
    "- TorchText : 각종 텍스트 데이터셋 및 데이터 관리용 도구\n",
    "\n",
    "### 파이토치 API\n",
    "- torch.nn\n",
    "- torch.optim as optim : 경사하강법 알고리즘들\n",
    "- torch.nn.init : 텐서 초깃값 함수들\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision Dataset\n",
    "\n",
    "- Face\n",
    "    - CelebA (얼굴데이터셋)\n",
    "    \n",
    "- Image Classification\n",
    "    - CIFAR (사물 데이터셋)\n",
    "    - ImageNet\n",
    "    - MNIST\n",
    "        - EMNIST(알파벳 + 숫자)\n",
    "        - Fashion-MNIST\n",
    "        - KMNIST(고대 중세 일본문자)\n",
    "        - QMNIST\n",
    "    - LSUN (Scene classification)\n",
    "- Segmentation\n",
    "    - Cityscapes(도시 길거리 데이터셋, segmentation용),\n",
    "- Object Detection\n",
    "    - COCO, VOC (Captioning and Detection)\n",
    "- Image Description\n",
    "    - Flickr\n",
    "- Etc\n",
    "    - FakeData(Random Noise Data)\n",
    "    - HMDB51 (action classification dataset)\n",
    "    - Kinetics-400 / Omniglot / PhotoTour / Places365\n",
    "    - SBD / SBU / STL10 / SVHN / UCF101 / USPS\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Torch Tensor\n",
    "- torch는 numpy와 거의 비슷\n",
    "- torch.Tensor(data, dtype, device, requires_grad)\n",
    "- device : cpu, gpu / requires_grad : 미분값 저장 여부 (default : false)\n",
    "- **torch.no_grad() : 테스트 모드 (기울기 안구하는)**\n",
    "    - dropout 등의 작동안하기 때문에 속도가 좀더 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "t = torch.tensor(data = [[1,2],[3,4]], dtype = torch.float64, device = 'cpu', requires_grad = True)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arange, numpy(), reshape, broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]) torch.Size([9]) [0 1 2 3 4 5 6 7 8]\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([ 0,  3,  6,  9, 12, 15, 18, 21, 24]) tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16])\n"
     ]
    }
   ],
   "source": [
    "s = torch.arange(9)\n",
    "print(s, s.shape, s.numpy())\n",
    "print(s.reshape(3,3))\n",
    "print(s*3, s+s ) # broadcasting 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randn, zeros, ones, zeros_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5332, 0.8513],\n",
      "        [0.1716, 0.0839]]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]]) \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "randoms = torch.rand((2,2))\n",
    "zeros = torch.zeros((2,2))\n",
    "ones = torch.ones((2,2))\n",
    "zeros_like = torch.zeros_like(ones) # 닮고 싶은 matrix \n",
    "print(randoms, '\\n', zeros, '\\n', ones, '\\n', zeros_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add, view, from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([4, 1, 5], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(ones, 5))\n",
    "print(ones.view(-1)) \n",
    "print(torch.from_numpy(np.array([4,1,5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Torch Tensor Processing\n",
    "- 텐서의 원소수는 유지하되 모양과 차원을 조절\n",
    "- squeeze(), unsqueeze()\n",
    "- view() : 데이터 reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: tensor([[ 1,  2,  3],\n",
      "        [10, 11, 12]]) 데이터 size: torch.Size([2, 3]) 데이터 shape: torch.Size([2, 3]) 데이터 차원: 2 \n",
      "\n",
      "랭크늘리기\n",
      "데이터: tensor([[[ 1,  2,  3],\n",
      "         [10, 11, 12]]]) 데이터 size: torch.Size([1, 2, 3]) 데이터 shape: torch.Size([1, 2, 3]) 데이터 차원: 3 \n",
      "\n",
      "랭크줄이기\n",
      "데이터: tensor([[ 1,  2,  3],\n",
      "        [10, 11, 12]]) 데이터 size: torch.Size([2, 3]) 데이터 shape: torch.Size([2, 3]) 데이터 차원: 2 \n",
      "\n",
      "view(reshape\n",
      "데이터: tensor([[ 1,  2],\n",
      "        [ 3, 10],\n",
      "        [11, 12]]) 데이터 size: torch.Size([3, 2]) 데이터 shape: torch.Size([3, 2]) 데이터 차원: 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[10,11,12]])\n",
    "print('데이터:', x, '데이터 size:', x.size(), '데이터 shape:', x.shape, '데이터 차원:', x.ndimension(), '\\n')\n",
    "\n",
    "# 랭크 늘리기\n",
    "x = torch.unsqueeze(x,0)\n",
    "print('랭크늘리기')\n",
    "print('데이터:', x, '데이터 size:', x.size(), '데이터 shape:', x.shape, '데이터 차원:', x.ndimension(), '\\n')\n",
    "\n",
    "# 텐서 줄이기\n",
    "x = torch.squeeze(x)\n",
    "print('랭크줄이기')\n",
    "print('데이터:', x, '데이터 size:', x.size(), '데이터 shape:', x.shape, '데이터 차원:', x.ndimension(), '\\n')\n",
    "\n",
    "# view 함수 = reahpe역할\n",
    "x = x.view([3,2])\n",
    "print('view(reshape')\n",
    "print('데이터:', x, '데이터 size:', x.size(), '데이터 shape:', x.shape, '데이터 차원:', x.ndimension(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Torch Tensor multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      " tensor([[-0.9356,  0.8518,  1.4608],\n",
      "        [ 0.2092, -0.2177, -0.1555]]) \n",
      " x \n",
      " tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "b\n",
      " tensor([[ 0, -1],\n",
      "        [ 0,  0]])\n",
      "wx\n",
      " tensor([[ 8.9240, 10.3011],\n",
      "        [-1.2215, -1.3855]])\n",
      "result\n",
      " tensor([[ 8.9240,  9.3011],\n",
      "        [-1.2215, -1.3855]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3, dtype=torch.float)\n",
    "x = torch.tensor([[1.0, 2.0], [3.0,4.0], [5.0,6.0]]) # 3,2\n",
    "print('w\\n', w,'\\n x \\n', x)\n",
    "\n",
    "b = torch.randint(-1, 1, (2,2))\n",
    "print('b\\n', b)\n",
    "\n",
    "wx = torch.mm(w,x)\n",
    "print('wx\\n', wx)\n",
    "\n",
    "result = wx+b\n",
    "print('result\\n', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Torch.backward()\n",
    "- 오차를 수학 함수로 표현 한 후 미분하여 이 함수의 기울기를 구해 오차의 최소값이 있는 방향을 찾아내는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 tensor tensor([2.], requires_grad=True) tensor([3.], requires_grad=True)\n",
      "연산 그래프 계산 한 값 tensor([19.], grad_fn=<AddBackward0>)\n",
      "w로 미분한 값(a)은 tensor([10.])\n",
      "w로 미분한 값(a)은 tensor([21.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([2.0], requires_grad = True)\n",
    "w2 = torch.tensor([3.0], requires_grad=True)\n",
    "print('기본 tensor', w, w2)\n",
    "\n",
    "# 미분1\n",
    "loss = 2*(w**2) + 2*w + 7 # 2w^2 + 2w + 7 \n",
    "print('연산 그래프 계산 한 값 {}'.format(loss))\n",
    "\n",
    "loss.backward() # ==> 4w + 2\n",
    "print('w로 미분한 값(a)은 {}'.format(w.grad))\n",
    "\n",
    "# 미분2\n",
    "loss2 = 3*(w2**2) + 3*w2 + 7 # 3w^2 + 3w + 7 \n",
    "loss2.backward() # ==> 6w + 3\n",
    "\n",
    "print('w로 미분한 값(a)은 {}'.format(w2.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Torch.optim\n",
    "- 경사하강법을 적용한 오차를 줄이고 최적의 가중치와 편차를 근사\n",
    "- SGD :Stochatstic gradient descent, 한 번에 들어오는 데이터의 수대로 경사하강법 알고리즘을 적용\n",
    "- Linear Regression Model에서의 L1Loss와 SGD Optimizer를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\urse\\Anaconda3\\envs\\seonmin\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\urse\\Anaconda3\\envs\\seonmin\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\urse\\Anaconda3\\envs\\seonmin\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear(in_features=1, out_features=1, bias=True)\n",
      "Model.parameters :  <bound method Module.parameters of Linear(in_features=1, out_features=1, bias=True)>\n",
      "x, y, y_noise, shape torch.Size([100, 1]) torch.Size([100, 1]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init # 텐서 초깃값 주기 위한 함수들\n",
    "\n",
    "num_data = 100\n",
    "num_epoch = 500\n",
    "\n",
    "# (1) Define X, y, noise Tensor\n",
    "x = init.uniform_(torch.Tensor(num_data, 1), -10, 10)\n",
    "y = 2*x + 3\n",
    "\n",
    "noise = init.normal_(torch.FloatTensor(num_data, 1), std=1)\n",
    "y_noise = y+noise\n",
    "\n",
    "# (2) 선형 회귀 모델, loss, optimizer 정의\n",
    "model = nn.Linear(1,1) # (1개 x를 넣어서 1개의 y가 나오는) \n",
    "loss_func = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # model의 parameters에 대해서 최적화 하겠다는 것\n",
    "\n",
    "# 정답\n",
    "label = y_noise\n",
    "\n",
    "print('Model:', model)\n",
    "print('Model.parameters : ', model.parameters)\n",
    "print('x, y, y_noise, shape', x.shape, y.shape, y_noise.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8165)\n",
      "1.9748817682266235 2.8977646827697754\n",
      "tensor(0.8146)\n",
      "1.9764351844787598 2.9357707500457764\n",
      "tensor(0.8148)\n",
      "1.9765570163726807 2.9413716793060303\n",
      "tensor(0.8145)\n",
      "1.9752084016799927 2.941971778869629\n",
      "tensor(0.8146)\n",
      "1.975310206413269 2.941971778869629\n"
     ]
    }
   ],
   "source": [
    "# 순서\n",
    "## (1) zero_gard() 기울기 초기화\n",
    "## (2) model 결과 output 추출\n",
    "## (3) y_true, y_pred간의 loss추출\n",
    "## (4) loss.backward()로 loss 전파\n",
    "## (5) optimizer.step() 실제 가중치 반영 (model.parameters()에 리턴되는 변수들의 기울기에 학습률 곱하고 빼줌)\n",
    "## (6) 처음 유도했던 Tensor 2와 3 에 가깝게 나옴.\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad() # 지난번 계산했던 기울기 초기화 (0으로)\n",
    "    output = model(x)\n",
    "    \n",
    "    loss = loss_func(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(loss.data)\n",
    "        param_list = list(model.parameters())\n",
    "        print(param_list[0].item(), param_list[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
