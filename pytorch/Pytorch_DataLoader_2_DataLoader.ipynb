{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Dataset 클래스\n",
    "\n",
    "- torch.utils.data.Dataset 은 데이터셋을 나타내는 추상클래스\n",
    "- **Dataset을 상속하고 오버라이드 하여 사용해야 함**\n",
    "    - __len__ 은 데이터셋의 크기를 리턴\n",
    "    - __getitem__ 은 i번째 샘플을 찾는데 사용 -> 로드한 데이터를 차례차례 돌려줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset 클래스 원형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An abstract class representing a Dataset.\n",
    "# All other datasets should subclass it and should override '__len__', '__getitem__'\n",
    "# __len__ : that provides the size of the dataset\n",
    "# __getitem__ : supporting integer indexing in range from 0 to len(self)\n",
    "\n",
    "# Dataset 원형\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CustomDataset, CustomDataloader\n",
    "- Custom Class가 생성될 때 DataLoader에서 사용 할 수 있는 최소한의 준비\n",
    "    - (1) 파라미터 인자를 받아 변수에 할당\n",
    "    - (2) class name list 파일을 받아 load\n",
    "    - (3) Dataset 존재 유무 확인\n",
    "    - (4) Dataset parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_path = '../../datasets/VOCdevkit/VOC2012/'\n",
    "image_path, label_path = \"JPEGImages/\", \"Annotations/\"\n",
    "img_extension, label_extension = '*.jpg', '*.xml'\n",
    "\n",
    "class VOC(Dataset):\n",
    "    def __init__(self, root, image_path, label_path,\n",
    "                 transform=None, target_transform=None,\n",
    "                 is_train=True, resize=300,):\n",
    "        self.root = root # 세부 폴더 전 root 데이터셋 경로\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        \n",
    "        self.transform = transform # image transform\n",
    "        self.target_transform = target_transform # label transform\n",
    "\n",
    "        self.is_train = is_train # train 여부\n",
    "        self.resize_factor = resize\n",
    "        \n",
    "        self.img_data = glob.glob(root_path + image_path + img_extension)\n",
    "        self.target_data = glob.glob(root_path + label_path + label_extension)\n",
    "\n",
    "        # dataset 이 root 경로에 존재하는지\n",
    "        if os.path.exists(os.path.join(self.root, self.image_path)) and \\\n",
    "            os.path.exists(os.path.join(self.root, self.label_path)):\n",
    "            print('Dataset is exist')\n",
    "        else:\n",
    "            raise RuntimeError(\"Dataset not found\")\n",
    "\n",
    "    # 전체 이미지 갯수 반환\n",
    "    def __len__(self):\n",
    "        return min(len(self.img_data), len(self.target_data))\n",
    "\n",
    "    # image 무조건 1장\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_data[index]).convert('RGB')\n",
    "        img = img.resize((self.resize_factor, self.resize_factor))\n",
    "\n",
    "        target = self.target_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform:\n",
    "            pass\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run CustomDataset, CustomDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is exist\n",
      "dataset length 5138\n",
      "0 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_003578.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_001370.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005342.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006325.xml')\n",
      "1 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_006841.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006941.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_002151.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005105.xml')\n",
      "2 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_002641.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000046.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005108.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_003143.xml')\n",
      "3 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_001165.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006261.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006185.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_001804.xml')\n",
      "4 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_006785.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006892.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_004417.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_002363.xml')\n",
      "5 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2008_002896.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_000046.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006196.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_002965.xml')\n",
      "6 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000197.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_004944.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_001456.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_003935.xml')\n",
      "7 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005344.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_006980.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000189.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_003261.xml')\n",
      "8 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_002894.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_007020.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_003235.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_002821.xml')\n",
      "9 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005858.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_003329.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_004954.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000654.xml')\n",
      "10 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_006875.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2010_006449.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_001051.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2008_000618.xml')\n",
      "11 torch.Size([4, 3, 300, 300]) ('../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_005147.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000608.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2012_000732.xml', '../../datasets/VOCdevkit/VOC2012/Annotations\\\\2011_003321.xml')\n"
     ]
    }
   ],
   "source": [
    "voc = VOC(ROOT_PATH, image_path, label_path, \n",
    "          transform = transforms.Compose([transforms.ToTensor()]),\n",
    "          target_transform = transforms.Compose([transforms.ToTensor()]),\n",
    "          is_train=True, resize=300,)\n",
    "    \n",
    "print('dataset length', dataloader.dataset.__len__())\n",
    "dataloader = DataLoader(voc, batch_size=4, shuffle=True)\n",
    "for i, (img, target) in enumerate(dataloader):\n",
    "    print(i, img.shape, target)\n",
    "    if (i+1)%12==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://deepbaksuvision.github.io/Modu_ObjectDetection/posts/03_01_dataloader.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
