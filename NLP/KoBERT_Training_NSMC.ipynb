{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778a113f",
   "metadata": {},
   "source": [
    "## 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e5908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9b0661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65adfb5",
   "metadata": {},
   "source": [
    "## Get Model / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac302cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# bert model\n",
    "# vocab - Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "tokenizer = get_tokenizer() # str => /home/neuralworks/kobert/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# tok('안녕하세요. 저는 김선민 입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552dfc56",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5df92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gluonnlp.data.dataset.TSVDataset'>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"data/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"data/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "print(type(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbdeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64 # 텍스트 데이터 최대 길이\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d43078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    # dataset, 0, 1, tokenizer, max_len, True, False\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f0e6e",
   "metadata": {},
   "source": [
    "## Modeling, Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b30e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa1e46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2, # 긍정 or 부정\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "    \n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a04a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer, loss\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# warmup scheduler\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# calc accuracy\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72751ab8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "368be9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2102643/1192263338.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3b008f0c0d4b9c9e63f9f617eda51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 batch id 1 loss 0.6857167482376099 train acc 0.65625\n",
      "epoch 1 batch id 201 loss 0.3573457896709442 train acc 0.7815609452736318\n",
      "epoch 1 batch id 401 loss 0.21133974194526672 train acc 0.8485037406483791\n",
      "epoch 1 batch id 601 loss 0.16963686048984528 train acc 0.8707882695507487\n",
      "epoch 1 batch id 801 loss 0.34017372131347656 train acc 0.8815933208489388\n",
      "epoch 1 batch id 1001 loss 0.37174472212791443 train acc 0.887987012987013\n",
      "epoch 1 batch id 1201 loss 0.46989017724990845 train acc 0.8932920482930891\n",
      "epoch 1 batch id 1401 loss 0.23267492651939392 train acc 0.8973724125624554\n",
      "epoch 1 batch id 1601 loss 0.3727855086326599 train acc 0.8999843847595252\n",
      "epoch 1 batch id 1801 loss 0.25608670711517334 train acc 0.9022938645197113\n",
      "epoch 1 batch id 2001 loss 0.2297532558441162 train acc 0.9024394052973513\n",
      "epoch 1 batch id 2201 loss 0.31833118200302124 train acc 0.9032400045433894\n",
      "epoch 1 batch id 2401 loss 0.2617761194705963 train acc 0.9035297792586422\n",
      "epoch 1 batch id 2601 loss 0.20992669463157654 train acc 0.9030541138023837\n",
      "epoch 1 batch id 2801 loss 0.02098619006574154 train acc 0.90265753302392\n",
      "epoch 1 batch id 3001 loss 0.3150902986526489 train acc 0.901647367544152\n",
      "epoch 1 batch id 3201 loss 0.2578926980495453 train acc 0.9009293970634177\n",
      "epoch 1 batch id 3401 loss 0.25113624334335327 train acc 0.9000477800646869\n",
      "epoch 1 batch id 3601 loss 0.20440344512462616 train acc 0.8989516800888642\n",
      "epoch 1 batch id 3801 loss 0.3017619848251343 train acc 0.897938042620363\n",
      "epoch 1 batch id 4001 loss 0.17986060678958893 train acc 0.8970647963009247\n",
      "epoch 1 batch id 4201 loss 0.3125690519809723 train acc 0.8962151868602714\n",
      "epoch 1 batch id 4401 loss 0.5032874345779419 train acc 0.8953859918200409\n",
      "epoch 1 batch id 4601 loss 0.23446260392665863 train acc 0.8947375570528147\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 train acc 0.894464590443686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2102643/1192263338.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5411ba23cca64c72ac99858e43cc876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 test acc 0.8825175943698017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5cf7f5a1b146a3a0cc7824049e2efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 2 batch id 1 loss 0.583040177822113 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.2601645588874817 train acc 0.8849502487562189\n",
      "epoch 2 batch id 401 loss 0.28650760650634766 train acc 0.8901184538653366\n",
      "epoch 2 batch id 601 loss 0.135768324136734 train acc 0.8953826955074875\n",
      "epoch 2 batch id 801 loss 0.3039056062698364 train acc 0.9001638576779026\n",
      "epoch 2 batch id 1001 loss 0.30369770526885986 train acc 0.9031905594405595\n",
      "epoch 2 batch id 1201 loss 0.5312280654907227 train acc 0.9064581598667777\n",
      "epoch 2 batch id 1401 loss 0.234229177236557 train acc 0.9085028551034975\n",
      "epoch 2 batch id 1601 loss 0.5139093399047852 train acc 0.9107784197376639\n",
      "epoch 2 batch id 1801 loss 0.2055889368057251 train acc 0.9119065796779567\n",
      "epoch 2 batch id 2001 loss 0.11262603849172592 train acc 0.9133714392803598\n",
      "epoch 2 batch id 2201 loss 0.45891839265823364 train acc 0.9144706951385734\n",
      "epoch 2 batch id 2401 loss 0.24356284737586975 train acc 0.9152436484798001\n",
      "epoch 2 batch id 2601 loss 0.11720920354127884 train acc 0.9157175124951942\n",
      "epoch 2 batch id 2801 loss 0.06756643205881119 train acc 0.9160009817922171\n",
      "epoch 2 batch id 3001 loss 0.1524130403995514 train acc 0.9163716261246251\n",
      "epoch 2 batch id 3201 loss 0.3676449656486511 train acc 0.9165885660731021\n",
      "epoch 2 batch id 3401 loss 0.24627703428268433 train acc 0.9167708027050867\n",
      "epoch 2 batch id 3601 loss 0.07194667309522629 train acc 0.9169848653151902\n",
      "epoch 2 batch id 3801 loss 0.12088023871183395 train acc 0.9173326098395159\n",
      "epoch 2 batch id 4001 loss 0.09963008016347885 train acc 0.9173409772556861\n",
      "epoch 2 batch id 4201 loss 0.31406170129776 train acc 0.9173708640799809\n",
      "epoch 2 batch id 4401 loss 0.4102141261100769 train acc 0.9173767325607817\n",
      "epoch 2 batch id 4601 loss 0.20763707160949707 train acc 0.9174092588567703\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 2 train acc 0.9174954671501706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e33631ce584300aed62f6f10397cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 2 test acc 0.8856365962891874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62033207c884bc3be95385f5809a280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 3 batch id 1 loss 0.6300756335258484 train acc 0.84375\n",
      "epoch 3 batch id 201 loss 0.1997765600681305 train acc 0.9264614427860697\n",
      "epoch 3 batch id 401 loss 0.13893330097198486 train acc 0.9281483790523691\n",
      "epoch 3 batch id 601 loss 0.10090045630931854 train acc 0.9331322795341098\n",
      "epoch 3 batch id 801 loss 0.2151477187871933 train acc 0.9355493133583022\n",
      "epoch 3 batch id 1001 loss 0.24839629232883453 train acc 0.9371565934065934\n",
      "epoch 3 batch id 1201 loss 0.45139408111572266 train acc 0.9390091590341382\n",
      "epoch 3 batch id 1401 loss 0.06209949031472206 train acc 0.9403327980014276\n",
      "epoch 3 batch id 1601 loss 0.5030152797698975 train acc 0.9405449718925671\n",
      "epoch 3 batch id 1801 loss 0.1697286069393158 train acc 0.9416470016657412\n",
      "epoch 3 batch id 2001 loss 0.17569869756698608 train acc 0.9425443528235882\n",
      "epoch 3 batch id 2201 loss 0.23497772216796875 train acc 0.9434064061790095\n",
      "epoch 3 batch id 2401 loss 0.09604765474796295 train acc 0.9442680133277801\n",
      "epoch 3 batch id 2601 loss 0.01851213350892067 train acc 0.9450932333717801\n",
      "epoch 3 batch id 2801 loss 0.0506800152361393 train acc 0.9454212781149589\n",
      "epoch 3 batch id 3001 loss 0.01728609949350357 train acc 0.9460075808063979\n",
      "epoch 3 batch id 3201 loss 0.2097991704940796 train acc 0.9460813027179007\n",
      "epoch 3 batch id 3401 loss 0.2721139192581177 train acc 0.9464128197588945\n",
      "epoch 3 batch id 3601 loss 0.0406053327023983 train acc 0.9466294084976395\n",
      "epoch 3 batch id 3801 loss 0.1604798138141632 train acc 0.9468560905024993\n",
      "epoch 3 batch id 4001 loss 0.03782656043767929 train acc 0.9471304048987753\n",
      "epoch 3 batch id 4201 loss 0.12116427719593048 train acc 0.9471405617710069\n",
      "epoch 3 batch id 4401 loss 0.260373592376709 train acc 0.9472137014314929\n",
      "epoch 3 batch id 4601 loss 0.1623142957687378 train acc 0.9474231145403174\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 3 train acc 0.9475655930034129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e1731ec9434c4d9b4b249388b17aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 3 test acc 0.8914747280870121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67f083e0aff47dda4e6ce77f7fc66b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 4 batch id 1 loss 0.5689281225204468 train acc 0.84375\n",
      "epoch 4 batch id 201 loss 0.14891383051872253 train acc 0.9500932835820896\n",
      "epoch 4 batch id 401 loss 0.06914684176445007 train acc 0.9559694513715711\n",
      "epoch 4 batch id 601 loss 0.018289947882294655 train acc 0.9597545757071547\n",
      "epoch 4 batch id 801 loss 0.09229589253664017 train acc 0.9620006242197253\n",
      "epoch 4 batch id 1001 loss 0.08820468187332153 train acc 0.963317932067932\n",
      "epoch 4 batch id 1201 loss 0.23846440017223358 train acc 0.9648209825145712\n",
      "epoch 4 batch id 1401 loss 0.18070903420448303 train acc 0.9654264810849393\n",
      "epoch 4 batch id 1601 loss 0.4838614761829376 train acc 0.9658611805121798\n",
      "epoch 4 batch id 1801 loss 0.22528715431690216 train acc 0.966129927817879\n",
      "epoch 4 batch id 2001 loss 0.10351048409938812 train acc 0.9669384057971014\n",
      "epoch 4 batch id 2201 loss 0.2027692198753357 train acc 0.9674295774647887\n",
      "epoch 4 batch id 2401 loss 0.08051129430532455 train acc 0.9678649521032903\n",
      "epoch 4 batch id 2601 loss 0.004899054765701294 train acc 0.968269415609381\n",
      "epoch 4 batch id 2801 loss 0.009874504990875721 train acc 0.96857149232417\n",
      "epoch 4 batch id 3001 loss 0.010098816826939583 train acc 0.9686042152615795\n",
      "epoch 4 batch id 3201 loss 0.15080760419368744 train acc 0.9686523742580444\n",
      "epoch 4 batch id 3401 loss 0.16633273661136627 train acc 0.9689429579535431\n",
      "epoch 4 batch id 3601 loss 0.018714504316449165 train acc 0.969270688697584\n",
      "epoch 4 batch id 3801 loss 0.14184559881687164 train acc 0.9693912786108919\n",
      "epoch 4 batch id 4001 loss 0.010135793127119541 train acc 0.9695466758310423\n",
      "epoch 4 batch id 4201 loss 0.015503749251365662 train acc 0.9695161866222328\n",
      "epoch 4 batch id 4401 loss 0.20377159118652344 train acc 0.9695097705067031\n",
      "epoch 4 batch id 4601 loss 0.026365084573626518 train acc 0.969435992175614\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 4 train acc 0.9694899210750854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6574a0dc784db4bddc6689b291ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 4 test acc 0.8943338131797824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fca8e326bf043c485c757f30b9c6538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 5 batch id 1 loss 0.4382215142250061 train acc 0.875\n",
      "epoch 5 batch id 201 loss 0.02459258772432804 train acc 0.970771144278607\n",
      "epoch 5 batch id 401 loss 0.07532317191362381 train acc 0.9747506234413965\n",
      "epoch 5 batch id 601 loss 0.004637721460312605 train acc 0.9773294509151415\n",
      "epoch 5 batch id 801 loss 0.1279895156621933 train acc 0.9777621722846442\n",
      "epoch 5 batch id 1001 loss 0.1061367616057396 train acc 0.9782405094905094\n",
      "epoch 5 batch id 1201 loss 0.25959092378616333 train acc 0.979131973355537\n",
      "epoch 5 batch id 1401 loss 0.007385326083749533 train acc 0.979768915060671\n",
      "epoch 5 batch id 1601 loss 0.379783570766449 train acc 0.9799539350405996\n",
      "epoch 5 batch id 1801 loss 0.11161907762289047 train acc 0.9800631593559134\n",
      "epoch 5 batch id 2001 loss 0.0036753620952367783 train acc 0.9805878310844578\n",
      "epoch 5 batch id 2201 loss 0.19590197503566742 train acc 0.9808183780099955\n",
      "epoch 5 batch id 2401 loss 0.0034861634485423565 train acc 0.9810755935027072\n",
      "epoch 5 batch id 2601 loss 0.0028569865971803665 train acc 0.9811971357170319\n",
      "epoch 5 batch id 2801 loss 0.005617042537778616 train acc 0.9813571046054981\n",
      "epoch 5 batch id 3001 loss 0.005705478601157665 train acc 0.9811417027657447\n",
      "epoch 5 batch id 3201 loss 0.006613994017243385 train acc 0.9812753826929085\n",
      "epoch 5 batch id 3401 loss 0.14151450991630554 train acc 0.9813290208762129\n",
      "epoch 5 batch id 3601 loss 0.003950390964746475 train acc 0.9812985976117745\n",
      "epoch 5 batch id 3801 loss 0.06252721697092056 train acc 0.9813207050776112\n",
      "epoch 5 batch id 4001 loss 0.008576678112149239 train acc 0.9813249812546864\n",
      "epoch 5 batch id 4201 loss 0.006490889471024275 train acc 0.9812990954534635\n",
      "epoch 5 batch id 4401 loss 0.19173991680145264 train acc 0.981176153147012\n",
      "epoch 5 batch id 4601 loss 0.005752259865403175 train acc 0.9811861551836557\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 5 train acc 0.9811820072525598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde896cd88de4aebb3d84814acf12b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 5 test acc 0.8936140435060781\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    torch.save(model, 'kobert-nsmc-'+str(e)+'.pt')\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
