{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 모델 \n",
    "- Residual Net 카이밍 허 등 개발, 2015년 우승\n",
    "- 기존 20개층의 한계에서 152개 층 까지 늘림\n",
    "- ResNet 팀은 망을 100 layer 이상으로 깊게 하면서, 깊이에 따른 학습 효과를 얻을 수 있는 방법을 고민하였으며, 그 방법으로 Residual Learning이라는 방법을 발표.\n",
    "\n",
    "### ResNet의 배경\n",
    "- 1. 깊은 망의 문제점(Vanishing/Exploding Gradient 문제) \n",
    "    - CNN에서 파라미터 update를 할 때, gradient값이 너무 큰 값이거나 작은 값으로 포화되어 더 이상 변화가 없어 학습의 효과가 없어지거나, 학습 속도가 아주 느려지는 문제\n",
    "    - **문제를 피하기 위해 batch normalization, 파라미터의 초기값 설정 방법 개선 등의 기법들이 적용되지만 layer의 개수가 일정 수를 넘어가게 되면 한계가 발생**\n",
    "\n",
    "- 2. ResNet 팀의 배경 실험 (망이 깊어졌을 경우의 학습결과)\n",
    "    - 짧은 시간의 실험의 효과를 위해 **ImageNet보다는 간단한 CIFAR-10학습 데이터를 20-layer와 56-layer에 대하여 비교 실험 진행**\n",
    "    - 실험결과로 보아 **학습 오차와 테스트 오차 모두 56-layer의 결과가 20-layer보다 나쁘게 나오는 것을 확인**\n",
    "    \n",
    "![resnet1](img/resnet1.png)\n",
    "\n",
    "### ResNet의 등장\n",
    "- 목표를 H(x)를 얻는 것이 아닌 출력과 입력의 차(H(x) - x)를 얻는 것으로 목표를 수정\n",
    "- 따라서 F(x) = H(x) - x 라면, 결과적으로 출력 H(x) = F(x) + x 가 된다, 그림에서의 F(x)는 그 뺀 차를 의미함.\n",
    "- 왼쪽 블록이 오른쪽 블록처럼 표현될 수 있으며, Residual Learning의 기본 블록\n",
    "\n",
    "![resnet_차이점](img/resnet_차이점.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차 모듈 (Residual Modeule)\n",
    "- **잔차 모듈 (residual module) : 기존신경망과 ResNet의 잔차 모듈 차이 즉 잔차를 학습함**\n",
    "- **잔차 모듈의 특징은 기대하는 출력과 유사한 입력이 들어오면 영벡터에 가까운 값을 학습 => 입력의 작은 변화에 민감 => 잔차 학습** \n",
    "- **다양한 경로를 통해 복합적인 특징 추출 하고 필요한 출력이 얻어지면 컨볼루션 층을 건너뛸 수 있음. 또한 다양한 조합의 특징 추출 가능**\n",
    "- Residual = (skip connection 이라고도 함\n",
    "- shortcut은 파라미터가 없이 바로 연결이 되는 구조여서 연산량 관점에서는 덧셈이 추가되는 것 외에는 차이가 없다. (F(x) = H(x) + x)\n",
    "- **입력에서 바로 출력으로 연결되는 shortcut 연결이 생기게 됨**\n",
    "\n",
    "\n",
    "### Resnet 효과 (identity shortcut)\n",
    "- (1) 이전에는 H(x)를 얻기 위한 학습 / 이제는 H(x) - x를 얻기 위한 학습을 하게 됨.\n",
    "- (2) 최적의 경우 F(x)는 0이며, 학습할 방향이 미리 결정이 되고, 이것이 pre-conditioning 구실을 하게 됨\n",
    "- (3) F(x)가 거의 0이 되는 방향으로 학습을 하게 되면, 입력의 작은 움직임(fluctuation)을 쉽게 검출 할 수 있다.\n",
    "- (4) 입력과 같은 x가 그대로 출력에 연결되기 때문에 파라미터의 수에 영향이 없으며, 덧셈이 늘어나는 것을 제외하면 shortcut 연결을 통한 연산량 증가는 없다. \n",
    "- (5) 몇 개의 layer를 건너 뛰면서 입력과 출력이 연결이 되기 때문에 forward나 backward path가 단순해지는 효과를 얻을 수 있다.\n",
    "\n",
    "### ResNet 구조\n",
    "- 망 설계시 VGGNet의 설계 철학을 많이 이용\n",
    "- 대부분의 convolutional layer는 3x3 kernel을 갖도록 하였음\n",
    "- 복잡도(연산량)을 줄이기 위해 max-pooling(1곳 제외), hidden fc, dropout등은 사용하지 않았음.\n",
    "\n",
    "![resnet](img/resnet.png)\n",
    "\n",
    "![resnet_structure](img/resnet_structure.png)\n",
    "\n",
    "### Resnet Bottleneck Architecture\n",
    "- 학습에 걸리는 시간을 고려하여 50, 101, 152 layer에 대해서는 기본구조를 조금 변경\n",
    "- Dimension Reduction & Expansion\n",
    "\n",
    "![Resnet_Bottleneck_Architecture](img/Resnet_Bottleneck_Architecture.png)\n",
    "\n",
    "### 차후\n",
    "- 테스트 시간과 효율을 위해 추가적으로 cifar10 으로 딥하게 layer쌓아봄"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ResNet 후속논문\n",
    "- Identity Mappings in Deep Residual Networks\n",
    "- ResNet에 적용된 Identity Mapping의 성공을 수식을 통해 분석하고, 최적의 Residual Network 구조는 무엇인지에 대한 실험을 진행\n",
    "\n",
    "### Identify Mapping 이란\n",
    "- **전체적으로 기존에 나왔던 Residual Network(이하 ResNet)이 왜 좋은 성능이 나오나 검증하는 논문의 성격을 띔.**\n",
    "- **또한 논문에서 기존의 논문이 제시한 구조 외의 이런저런 변형을 가했을 때 성능이 어떻게 변하는지도 검증함**\n",
    "- Identify Mapping이란 네트워크의 shortcut으로 구현\n",
    "- 이를 바탕으로 Feed Forwarding과 Backpropagation시에 직접적인 전파를 가능\n",
    "- 이러한 ＇Clean＇한 정보의 통로는 최적화에 도움이 됨\n",
    "\n",
    "- **즉 일반적인 CNN의 경우에는 최종 결과값이 수많은 행렬들의 곱셈으로 표현되는데, ResNet은 이 Clean한 루트인 Shortcut 덕분에 결과값을 간단히 Residual Unit들의 덧셈으로 표현할 수 있게 되어, Backpropagation시에도 Vanishing 문제가 일어나지 않는다.**\n",
    "\n",
    "### 다양한 Identify Mapping 실험 - shortcut\n",
    "![identify_mapping](img/identify_mapping.jpg)\n",
    "\n",
    "- Constant Scaling : F에는 적용하지 않은 경우, 1−λ=0.5  만큼 적용한 경우로 구분 가능\n",
    "\n",
    "- Gating의 경우에는 g(x)=σ(Wgx+bg)를 적용한 것, 여기서 g(x)g(x)는 1x1 Conv와 Sigmoid를 의미함.\n",
    "\n",
    "- Exclusive gating의 경우에는 F는 g(x)만큼 곱해주고(element-wise), shorcut path에는 1−g(x)를 곱해준 것 shorcut-only gating의 경우에는 shorcut path만 1−g(x)를 적용.\n",
    "\n",
    "- shortcut에 1x1Conv를 적용하거나 dropout을 적용하고 실험을 진행\n",
    "\n",
    "- **결과적으로 봤을 때 shortcut의 정보를 훼손시키지 않는 것이 제일 좋다고 함.**\n",
    "- **같은말로 shortcut path에 어떤 곱 연산을 시도하면 최적화하는데 방해가 된다는 것 – 그냥 더하기만 하게 냅두기 **\n",
    "![identify_mapping_shortcut](img/identify_mapping_shortcut.jpg)\n",
    "\n",
    "\n",
    "### 다양한 Identify Mapping 실험 - activation\n",
    "- Shortcut path를 손상시키지 않는 것이 가장 좋다는 것을 알았고, 활성화함수에 대한 실험을 진행\n",
    "\n",
    "![identify_mapping_activation](img/identify_mapping_activation.png)\n",
    "- (a) 원래 것은 BN 까지 진행 후 Addition 후에 ReLU 를 입힘.\n",
    "- (b) BN 나중에\n",
    "- (c) BN, Relu 다 하고 addition\n",
    "- (d) Relu 를 먼저\n",
    "- (e) BN, Relu 먼저\n",
    "\n",
    "![identify_mapping_activation_result](img/identify_mapping_activation_result.jpg)\n",
    "- **활성화함수에서는 pre-activation을 적용한 모델의 경우가 더 낮았음.**\n",
    "- 단 기본 모델보다 training set에 대한 정확도는 낮지만, test에 대한 정확도는 더 높았고, 이러한 결과를 해석 하길\n",
    "    - BN이 모델의 regularization 역할을 수행했다. \n",
    "    - pre-activation을 적용한 경우 최적화가 훨씬 쉬워진 것 이라고 주장\n",
    "    \n",
    "![resnet_preactivation](img/resnet_preactivation.PNG)\n",
    "\n",
    "### 후속논문 결론\n",
    "- Shorcut path의 정보는 가능한 손상시키지 않는 것이 역전파, 정보의 전달 측면에서 유리하다\n",
    "- Residual path에서는 shortcut과 합쳐 주기 전에 activation을 취해주는 것이 조금 더 유리하다.\n",
    "\n",
    "- 따라서 ResNet을 조금 더 개량한 오른쪽 모형을 제안\n",
    "\n",
    "![identify_mapping_result](img/identify_mapping_result.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "- https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet\n",
    "- https://blog.naver.com/laonple/220770760226"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
