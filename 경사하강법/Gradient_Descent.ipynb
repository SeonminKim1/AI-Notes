{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 경사하강법 (Gradient Descent)\n",
    "- 경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜서 극값에 이를 때까지 반복시키는 것이다.(위키백과 정의)\n",
    "\n",
    "- 경사하강법의 목표는 전역 최댓값에 도달하는 것으로 머신러닝 모델에서 흔히 사용되는 **최적화 기술**이다.\n",
    "\n",
    "- 비용함수가 convex 함수\n",
    "\n",
    "- 비선형 활성화 함수를 사용하는 ANN 은 극솟값을 가지기 때문에 가능하다. (선형일 경우는 극솟값이란게 존재 x)\n",
    "\n",
    "- 경사하강법을 사용할 때는 ㅁ모든 특성이 같은 스케일을 갖도록 만드는게 좋음\n",
    "\n",
    "![경사하강법1](img/경사하강법1.png)\n",
    "- cost function이 y축이며 w가 x축 -> 가중치에 따라 비용이 최소가 될 때를 찾아야 됨\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ■ 경사하강법 종류\n",
    "- Gradinet Descent Learning 에는 기본적으로 3가지 타입이 존재\n",
    "- *** 경사 하강법을 구현시 각 모델 파라미터 Θ에 대해 비용 함수의 그래디언트를 계산해야 한다. (==> 즉 세타가 변경될 때 비용 함수가 얼마나 바뀌는지) = 편도함수(partial derivative)\n",
    "- **경사하강법 자체가 Local Minimize에 빠질 우려가 있다. Global Minimize점을 찾지 못하고**\n",
    "\n",
    "![경사하강법2](img/경사하강법2.PNG)\n",
    "\n",
    "### ■ 경사하강법 종류 (1) 배치 경사 하강법 (Batch Gradient Descent)\n",
    "- 배치 경사 하강법\n",
    "    - 배치 경사 하강법은 동쪽을 바라봤을 때 발밑의 산의 기울기는 얼마인가? 그리고 북쪽은 얼마인가? 남서쪽은 얼마인가에 대한 모든 차원의 반복\n",
    "    - 즉 Gradient Vecotr는 비용 함수의 (모델 파라미터 마다 1개씩)인 편도함수를 모두 담고 있음\n",
    "    - **파라미터를 업데이트 할 때마다 모든 학습 데이터를 사용하여 cost function의 gradient를 계산, vanilla gradient decent 라고도 불림**\n",
    "    - 문제점 : 매 스텝에서 전체 훈련 세트를 사용해 그래디언트를 계산하기 때문에 매우 느려지게 됨\n",
    "![비용함수의그래디언트벡터](img/비용함수의그래디언트벡터.png)\n",
    "\n",
    "### ■ 경사하강법 종류 (2) 확률적 경사하강법 (Stochastic Gradient Descent - SGD)\n",
    "- SGD 경사하강법\n",
    "    - 파라미터를 업데이트 할 때, 무작위로 샘플링된 학습 데이터를 하나씩만 이용하여 cost function의 gradient를 계산\n",
    "    - 매 스텝에서 딱 한개의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련 가능하다\n",
    "    - 모델을 훨씬 더 자주 업데이트 하며, 성능 개선 정도를 빠르게 확인 가능\n",
    "    - Local minimaize에 빠질 가능성을 줄일 수 있음 (무작위성으로 인한 지역 최솟값을 지나칠 수 있다)\n",
    "\n",
    "\n",
    "- 문제점\n",
    "    - 확률적 이기 때문에 배치 경사 하강법보다 훨씬 불안정함\n",
    "    - 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치면서 평균적으로 감소하지만, 속도가 확연히 빠르고 최적 해에 근사한 값을 찾아낸다는 장점덕분에 경사하강법의 대안으로 사용되고 있음\n",
    "    - Localminimize 를 벗어나지만 Global Minimize에 다다르지 못할 수도 있다. -> 학습률을 점진적으로 감소시키는 방법을 사용 (학습스케쥴을 짠다고 함)\n",
    "\n",
    "### ■ 경사하강법 종류 (3) 미니배치 경사하강법 (Mini-batch Gradient Descent)\n",
    "- 미니배치 경사하강법 (Mini batch SGD 라고도 함)\n",
    "    - 하나의 샘플이 아닌 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레디언트롤 계산\n",
    "    - 파라미터를 업데이트 할 때마다, 일정량의 일부 데이터를 무작위로 뽑아 cost function의 gradient를 계산\n",
    "    - Batch gradient와 Stochastic gradient descent 개념의 혼합\n",
    "    - SGD 노이즈를 줄이면서 , 전체 배치보다 효율적 (가장 널리 사용되는 기법)\n",
    "    - 미니배치 경사 하강법의 주요 장점은 행렬 연사에 최적화된 하드웨어, 특히 gpu를 사용해서 얻는 성능 향상이다.\n",
    "    - 미니배치를 사용시 파라미터 공간에서 SGD보다 덜 불규칙적으로 움직이나 지역 최솟값에서 빠져나오기는 조금더 힘들어 질지도 모름\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## ■ 속도와 정확도 문제를 해결하는 고급 경사 하강법 \n",
    "- **확률적 경사하강법(SGD), 모멘텀, NAG, Adagrad, RMSProp** \n",
    "- Momentum, NAG, Adagrad, AdaDelta, RMSprop 등은 SGD의 변형이다. 거의 모든 경우에서 SGD는 다른 알고리즘들에 비해 성능이 월등하게 낮다\n",
    "\n",
    "### ■ 고급경사하강법 종류 (1) 모멘텀 (Momentum)\n",
    "\n",
    "![momentum](img/momentum.png)\n",
    "\n",
    "- Momentum Optimization(모멘텀 최적화)\n",
    "    - 볼링공이 매끈한 표면의 완만한 경사를 따라 굴러간다고 할 때 처음에는 느리게 출발하지만 종단속도에 도달할 때까지는 빠르게 가속 될 것임 => 모멘텀 최적화 원리\n",
    "    - 일반적인 경사하강법은 경사면을 따라 일정한 크기의 스텝으로 조금씩 내려가는데, 맨 아래에 도착하는 시간이 더 오래 걸릴 것이다.\n",
    "    - **모멘텀 최적화는 이전 그래디언트가 얼마였는지를 중요하게 생각하여, 매 반복에서 현재 그래디언트를 (학습률을 곱한 후, 모멘텀 벡터 m 에 더하고 이 값을 빼는 방식으로 가중치를 갱신한다** \n",
    "    - 그래디언트를 속도가 아니라 가속도로 사용하고,일종의 마찰저항을 표현해 모멘텀이 너무 커지는 것을 막기 위해 새로운 하이퍼 파라미터 모멘텀 (β = 마찰저항 = 모멘텀값)이 등장\n",
    "    - **즉 모멘텀 최적화가 경사 하강법보다 더 빠르게 평평한 지역을 탈출하게 도와준다!!!!!**\n",
    "    - 관성을 이용하여 Local minimum과 잡음에 대처할 수 있음. (이동 벡터를 사용하여 2배의 메모리를 사용)\n",
    "- 코드\n",
    "    - tf.train,MomentumOptimizer(learning_rate =learning_reate, momentum=0.9,\n",
    "    \n",
    "![모멘텀](img/모멘텀.png)\n",
    "\n",
    "- 모멘텀벡터 = (β(av) 마찰저항 = 0.9) - 학습률 * 비용함수\n",
    "- 가중치 갱신\n",
    "\n",
    "- **모멘텀 최적화의 한 가지 단점은 튜닝할 하이퍼 파라미터가 하나 늘어난다는 것인데, 일반적으로 모멘텀 = 0.9에서 보통 잘 작동하며 경사 하강법보다 거의 더 빠르다 !!!**\n",
    "\n",
    "### ■ 고급경사하강법 종류 (2) 네스테로프 가속 경사 (Nesterov Momentum Optimization = NAG)\n",
    "- 네스테로프 모멘텀 최적화 (Nesterov Momentum Optimization) = NAG\n",
    "    - 모멘텀 최적화의 한 변종으로 기본 모멘텀 최적화보다 거의 항상 더 빠르다\n",
    "    - 기본 아이디어는 현재 위치가 아니라 모멘텀의 방향으로조금 앞서서 비용 함수의 그래디언트를 계산하는 것\n",
    "    - 모멘텀최적화와 다른 점은 Θ가 아닌 Θ-β* m 이란 것\n",
    "    - 일반적으로 모멘텀 벡터가 올바른 방향 (즉 최적점을 향하는 방향)을 가리킬 것이므로 그 방향으로 조금 더 나아가서 측정한 그래디언트를 사용하는 것이 약간 더 정확할 것)\n",
    "\n",
    "![NAG](img/NAG.PNG)\n",
    "\n",
    "- 코드\n",
    "    - tf.train,MomentumOptimizer(learning_rate =learning_reate, momentum=0.9, use_nesterov=True) => use_nesterov = True 가 추가된 것\n",
    "\n",
    "- 모멘텀 VS 네스테로프 모멘텀 비교\n",
    "![모멘텀_네스테로프모멘텀](img/모멘텀_네스테로프모멘텀.png)\n",
    "\n",
    "\n",
    "### ■ 고급경사하강법 종류 (3) 에이다그라드 (AdaGrad = Adaptive Gradient)\n",
    "- Adagrad (Adaptive Gradient)\n",
    "    - AdaGrad : 각각의 매개변수에 맞춤형 학습률을 제공하는 방식\n",
    "    - 변수들을 update할 떄 각각의 변수마다 step size 를 다르게 설정하여 이동하는 방식\n",
    "    - 기본적인 아이디어는 '지금까지 많이 변화하지 않은 변수들은 step size를 크게 하고, 지금까지 많이 변화했던 변수들은 step size를 작게 하자' 이다.\n",
    "    - 자주 등장하거나 변화를 많이 한 변수들의 경우 최적지점에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 세밀한 값을 조정하고, 적게 변화한 변수들은 optimum 값에 도달하기 위해서 많이 이동해야할 확률이 높을 것이다.\n",
    "    - 가파른 차원에서는 학습률이 더 빠르게 감소하고, 완만한 차원에서는 더 증가한다.\n",
    "    \n",
    "    - 학습률 하이퍼 파라미터를 덜 튜닝해도 되는 점이 장점.\n",
    "- 문제점\n",
    "    - 너무 빠르게 느려져서 전역 최적점에 수렴하지 못한다.\n",
    "    - 선형 회귀 같은 간단 작업에서는 효과적이나, 신경망을 훈련시킬 때는 너무 일찍 멈취버리는 문제가 있음\n",
    "    - 따라서 텐서플로에 AdagradOptimizer가 있지만 심층 신경망에는 사용 x\n",
    "    \n",
    "![에이다그리드](img/에이다그리드.PNG)\n",
    "\n",
    "### ■ 고급경사하강법 종류 (4) RMSProp\n",
    "- RMSProp\n",
    "    - Adagrad의 너무 빠르게 느려져서 전역 최적점에 수렴하지 못하는 단점을 해결한 방법으로 훈련 시작부터의 모든 그래디언트가 아닌, 가장 최근 반복에서 비롯된 그래디언트만 누적하여 이 문제를 해결\n",
    "    - 일반적으로 모멘텀 최적화나 네스테로프 가속 경사보다 더 빠르게 수렴함.\n",
    "    - Adam 최적화 가 나오기전까지 가장 선호되는 최적화 알고리즘\n",
    "    - 합대신 지수평균 사용\n",
    "- 코드\n",
    "    - tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)\n",
    "    - 감쇠율은 0.9로 설정 후 거의 다 잘 작동\n",
    "    \n",
    "![rmsprop](img/rmsprop.PNG)\n",
    "\n",
    "### ■ 고급경사하강법 종류 (5) Adam (Adaptive Moment Estimation)\n",
    "- Adam ( Momentum + Adagrad) 적응적 모멘트 추정\n",
    "    - m = Momentum 방식과 유사하게 지금까지 계산해온 기울기의 지수 평균을 저장하며\n",
    "    - v = RMSProp 방식과 유사하게 기울기의 제곱값의 지수평균을 저장한다\n",
    "    - 다만 m과 v는 처음에 0으로 초기화 되어 있기 때문에 0에 가깝게 bias되있을 것으로 판단 unbiased 하게 만들어주는 작업을 걸침 - 10^-8\n",
    "    \n",
    "- 코드 \n",
    "    - tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    - 모멘텀 감쇠 하이퍼 파라미터는 = 0.9로 초기화, 스케일 감쇠 하이퍼 파리미터는 0.999로 초기화 하는것이 일반적이다. \n",
    "    - **따라서 코드가 매우 간단히 사용되며, 적응적 학습률 알고리즘이기 때문에 학습률 하이퍼파라미터를 튜닝할 필요 또한 적다 일반적으로 0.001 사용**\n",
    "\n",
    "![adam](img/adam.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ■ 경사하강법 선택\n",
    "\n",
    "![경사하강법선택](img/경사하강법선택.png)\n",
    "\n",
    "![산내려오는작은오솔길잘찾기](img/산내려오는작은오솔길잘찾기.jpg)\n",
    "\n",
    "### ■ Summary\n",
    "\n",
    "<table><thead><td>경사하강법</td><td>개요</td><td>효과</td></thead>\n",
    "\n",
    "<tr><td>확률적 경사 하강법(SGD)</td><td>랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 업데이트를 하게 하는 것</td><td>속도개선</td></tr>\n",
    "<tr><td>모멘텀</td><td>관성의 방향을 고려해 진동과 폭을 줄이는 효과</td><td>정확도개선</td></tr>\n",
    "<tr><td>네스테로프 모멘텀</td><td>모멘텀이 이동시킬 방향으로 미리 이동해서 그레이디언트를 계산. 불필요한 이동을 줄이는 효과</td><td>정확도개선</td></tr>\n",
    "<tr><td>아다그라드</td><td>변수의 업데이트가 잦으면 학습률을 적게 하여 이동 보폭을 조절하는 방법</td><td>보폭 크기 개선</td></tr>\n",
    "<tr><td>알엠에스프롭</td><td>아다그라드의 보폭 민감도를 보완한 방법</td>보폭 크기 개선<td></td></tr>\n",
    "<tr><td>아담</td><td>모멘텀과 알엠에스프롭 방법을 합친 방법</td><td>정확도, 보폭 크기 개선</td></tr>\n",
    "</table>\n",
    "\n",
    "### 참고문헌\n",
    "- 핸즈온 머신러닝\n",
    "- https://bioinformaticsandme.tistory.com/134 \n",
    "- http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
