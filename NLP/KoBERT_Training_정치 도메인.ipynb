{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ab3ed8",
   "metadata": {},
   "source": [
    "# KoBERT - Training 정치 도메인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6be1fd",
   "metadata": {},
   "source": [
    "## 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af5b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c6433",
   "metadata": {},
   "source": [
    "## 2. Get Pretrained-Model / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e9c37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# bert model\n",
    "# vocab - Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "tokenizer = get_tokenizer() # str => /home/neuralworks/kobert/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# tok('안녕하세요. 저는 김선민 입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1591259",
   "metadata": {},
   "source": [
    "## 3. 데이터 Processing \n",
    "- xlsx -> to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e4c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n",
      "train_data (428, 2), test_data (184, 2)\n",
      "train, test txt파일 저장완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('data/Youtube댓글정리_httpsyoutu.belm7p--F7eF0_이재명_모든경기도민에_재난지원금_정리완료.xlsx')\n",
    "data = data.iloc[:-4]\n",
    "\n",
    "# data one hot encoder\n",
    "ls = [{'positive':2,'neutral':1, 'negative':0}[i] for i in data['결과']]\n",
    "data['결과_인코더'] = ls\n",
    "\n",
    "# split train-test 7:3\n",
    "size = int(len(data)*0.7)\n",
    "print(size)\n",
    "train_policy = data[['댓글','결과_인코더']].iloc[:size]\n",
    "test_policy = data[['댓글','결과_인코더']].iloc[size:]\n",
    "print(f'train_data {train_policy.shape}, test_data {test_policy.shape}')\n",
    "\n",
    "# DataFrame 이나 Serises 를 txt 파일로 깔끔하게 바꿀경우 (이건 tsv)\n",
    "train_policy.to_csv('data/youtube_train_policy1.txt', index=False, header=None, sep=\"\\t\")\n",
    "test_policy.to_csv('data/youtube_test_policy1.txt', index=False, header=None, sep=\"\\t\")\n",
    "print('train, test txt파일 저장완료')\n",
    "\n",
    "#train_policy['결과_인코더'].value_counts() # 233, 150, 45\n",
    "#test_policy['결과_인코더'].value_counts() # 131, 35, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b246cf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    364\n",
       "positive    185\n",
       "neutral      63\n",
       "Name: 결과, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['결과'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b6eec",
   "metadata": {},
   "source": [
    "## 4. Get Dataset\n",
    "- txt 파일 TSV로 읽어들이기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0df56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# field indices=[1,2] 란 => [텍스트, 0] 값 지정한 것\n",
    "# num_discard_samples은 tsv 데이터 상위 몇 개의 row를 제거할지\n",
    "dataset_train = nlp.data.TSVDataset(\"data/youtube_train_policy1.txt\")\n",
    "dataset_test = nlp.data.TSVDataset(\"data/youtube_test_policy1.txt\")\n",
    "\n",
    "max_len = 64 # 텍스트 데이터 최대 길이\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df01c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    # dataset, 0, 1, tokenizer, max_len, True, False\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde673c",
   "metadata": {},
   "source": [
    "## 5. Modeling, Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee93353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 1\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26e63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=3, # 긍정 or 부정 or 중립\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "    \n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23a7a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer, loss\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# warmup scheduler\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# calc accuracy\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee9ee1",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cdde8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2643448/3349214014.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed4a30c8d6441e1a379da7e3c4c23d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 batch id 1 loss 0.9849988222122192 train acc 0.25\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 train acc 0.5794392523364486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2643448/3349214014.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52197d323fdb4877b40dfae8a52e2d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "The current process just got forked. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch 1 test acc 0.7119565217391305\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    torch.save(model, 'model/kobert-policy-'+str(e)+'.pt')\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
