{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 자연어 처리 (NLP) 관련 용어  \n",
    "- NLP (Natural Language Processing) : 자연어 처리\n",
    "- NLU (Natural Language Understanding) : 자연어 이해\n",
    "    - ex) 형태소분석, 의미분석, 구문분석\n",
    "- NLG (Natural Language Generation) : 자연어 생성 \n",
    "    - ex) 챗봇, 음성합성 등\n",
    "- NMT (Neural Machine Translation) : 자연어 번역\n",
    "    - 통계 기반 (SMT, Statistical Machine Translation)\n",
    "    - 규칙 기반 (RBMT, Rule Based Machine Translation)\n",
    "    - 뉴럴 기반 (거의 다 이쪽)\n",
    "    \n",
    "- NER (Named Entity Recognition) : 개채명인식\n",
    "- MRC (Machine Reading Comprehension): 기계번역, 자동통역, 기계독해\n",
    "- Sentiment Analysis : 감성분석\n",
    "- Dialog System : 챗봇, 일반채팅, 정보 요구 채팅 등\n",
    "- Question answering : 정보검색, 질의응답 시스템\n",
    "- TTS (Text to Speech)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 자연어 분석 단계\n",
    "\n",
    "### 2.1 형태소분석(Morphological Analysis)\n",
    "- 입력된 문자열을 분석하여 형태소(morpheme)라는 최소 의미 단위로 분리\n",
    "- 사전 정보와 형태소 결합 정보 이용\n",
    "- 정규 문법(Regular Grammar)으로 분석 가능\n",
    "\n",
    "### 2.2 구문(문법)분석(Syntax Analysis)\n",
    "- 문장의 구조적 성질을 규칙으로 표현한 것\n",
    "- 문법을 이용하여 문장의 구조를 찾아내는 process\n",
    "- 문장의 구문 구조는 Tree 형태로 표현가능\n",
    "    - ex) 즉, 몇 개의 형태소들이 모여서 구문 요소(구: phrase)를 이루고, 그 구문 요소들간의 결합구조를 Tree형태로써 구문 구조를 이루게 됨\n",
    "![syntax_analysis](img/syntax_analysis.PNG)\n",
    "\n",
    "### 2.3 의미분석(Semantic Analysis)\n",
    "- 통사 분석 결과에 해석을 가하여 문장이 가진 의미를 분석\n",
    "- 문법적으로는 맞지만 의미적으로 틀린 문장이 있을 수 있음\n",
    "    - ex) 돌이 걸어간다, 사람이 비행기를 먹는다\n",
    "![semantic_analysis](img/semantic_analysis.PNG)\n",
    "\n",
    "### 2.4 화용분석 (Pragmatic Analysis) (거의안씀 X)\n",
    "- 문장이 실세계(real world)와 가지는 연관관계 분석\n",
    "- 실세계 지식과 상식의 표현이 요구됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. 자연어 처리 임베딩(벡터화) 기법\n",
    "- 자연어 처리를 하기 위해선 텍스트를 컴퓨터가 이해할 수 있는 숫자로 바꾸어야 함\n",
    "- 과거) word representation 방법으로 one-hot encoding을 주로 사용\n",
    "    - 단어가 많아지면 차원이 매우 증가 (높은 dimension 문제)\n",
    "    - one hot은 단어들 간의 관계성 파악 힘듬 => Word Embedding 등장\n",
    "- 현재) Word Embedding (Vectorization)\n",
    "    - **단어를 과거의 희소 벡터가 아닌, Dense한 실수 벡터 공간 매핑**\n",
    "    - 유사한 의미의 단어는 벡터 공간 상, 가까운 거리내에 분포\n",
    "    - **유사도 계산을 통해 단어 간의 의미적ㆍ문법적 관계를 파악 가능**\n",
    "    - LSA, Word2vec, FastText, Glove 등.. 이 존재\n",
    "\n",
    "### 3.1 통계적 기반\n",
    "- 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "    - 단어 사용 빈도 등 말뭉치(corpus, 코퍼스)의 통계량 활용\n",
    "    - **코퍼스 통계량 정보 행렬에 특이값 분해 등 수학적 기법 사용**, 행렬에 속한 벡터들의 차원을 축소\n",
    "    - 차원 축소를 통해 얻은 행렬을 기존의 행렬과 비교시 단어 기준이면 단어 수준 임베딩, 문서 기준 문서 임베딩이 됨\n",
    "    - Term-Document, Bow(Bag-of-word), TF-IDF, One-Hot Encoding 등이 있음\n",
    "\n",
    "#### 3.1.1 Bow(Bag-of-words)\n",
    "- 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)로 수치화 하는 방법\n",
    "\n",
    "#### 3.1.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- 출현 빈도 기반, 문서에서 특정 단어가 얼마나 중요 역할을 하는지를 나타내는 통계적 수치\n",
    "\n",
    "#### 3.1.3 One-hot Encoding\n",
    "- 문자를 숫자로 표현하는 가장 기본적인 방법\n",
    "- 단어들 간의 유사성과 반대적 의미에 대해 반영 불가\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 뉴럴 네트워크 기반\n",
    "- 발전 순서\n",
    "    - NPLM -> Word2Vec(CBOW, Skip-Gram) -> Glove -> FastText -> Cove -> ELMO -> GPT -> BERT -> GPT-2 -> XLNET -> ROBERTa \n",
    "\n",
    "- 단어수준과 문장 수준이 존재\n",
    "- 단어수준\n",
    "    - 단어가 주어지면 그 단어와 주변 단어가 동시에 일어날 확률을 구하여, 단어의 의미를 수치화 하는 방식\n",
    "    - 이는 **비슷한 의미를 가진 단어는 크기와 방향에 유사성을 가지는 경향**이 있을 것이라는 가정\n",
    "    - Word2Vec, GloVe, FastText, ELMO 등\n",
    "\n",
    "![word_embedding_pros_and_cons](img/word_embedding_pros_and_cons.PNG)\n",
    "\n",
    "- 문장 수준\n",
    "    - ELMO 이후 개별 단어 => 단어 Sequence 전체의 문맥적 의미를 함축하기 위한 시도.\n",
    "    - Transfer Learning이 효과가 좋은 것으로 알려짐, 동음이의어도 문장 수준 임베딩 기법을 사용하여 분리 이해 가능\n",
    "    - BERT, GPT 등\n",
    "    \n",
    "#### 3.2.1 단어 수준 - Word2Vec\n",
    "- 주요 가정은 **비슷한 분포를 가진 단어이면 가까운 벡터로 표현된다**\n",
    "- 유사한 단어들은 벡터 공간상에서 Eucledian 거리나 코사인 유사도(cosine similarity 거리가 가까운 벡터들로 표현\n",
    "- Word2Vec은 단어를 쪼개질 수 없는 단위로 생각\n",
    "- Word2vec은 CBOW(Continuous Bag of Words )와 skipgram 두 가지 모델로 분류\n",
    "    - CBOW : 주변 단어(문맥)를 통해 중심 단어 유추하는 방법\n",
    "    - Skip-gram : 중심 단어를 통해 주변 단어를 예측하는 방법\n",
    "![word2vec](img/word2vec.PNG)\n",
    "    \n",
    "#### 3.2.2 단어수준 - Fast2Text (페이스북)\n",
    "- 단어를 개별 단어가 아닌 n-gram의 characters(Bag-Of-Characters)를 적용\n",
    "- **내부 단어(subword)를 고려 하나의 단어를 여러 개로 잘라 벡터 계산**\n",
    "- ex) apple => trigram => [app, ppl, ple]\n",
    "\n",
    "#### 3.2.3 단어수준 - ELMO (Embeddings from Language Model, 2018)\n",
    "- '언어 모델로 하는 임베딩' 라는 뜻으로, 사전 훈련된(Pre-trained mODEL)을 사용하여 임베딩\n",
    "    - ex) Bank Accuount, River Bank 구분시 word2vec은 두 Bank를 같은 벡터로 사용하는 한계점 존재 -> 동음이의어에 한계점\n",
    "- Transfer Learning 이 확산되는 계기가 됨 => BERT의 등장\n",
    "\n",
    "#### 3.2.4 문장 수준 - BERT (Bidirectional Encoder Representations from Transformer, 2018 구글)\n",
    "- 사전학습(pre-trained) 모델로, 특정 과제(task)를 하기 전 사전훈련 임베딩을 실시하여, 기존의 임베딩 기술보다 성능을 더욱 향상\n",
    "- 주요 특징\n",
    "    - 대량의 코퍼스를 인코드(Encoder)가 임베딩하고, 이를 트렌스퍼(Transfer)하여 Fine-tuning을 통해 목적에 맞는 학습을 수행\n",
    "    - BERT는 문장 전체를 입력받고 단어를 예측하고 양방향(bi- directional) 학습이 가능\n",
    "\n",
    "![bert](img/bert.PNG)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TODO\n",
    "- GPT : transformer의 decoder를 사용 / GPT-2 는 GPT와 유사 (좀더 데이터 많이 사용)\n",
    "- XLNET : Transformer XL 모델을 가지고 학습 한 것\n",
    "- ROBERTa : BERT 에 쓰이는 하이퍼 파라미터를 좀더 조정 한 것\n",
    "- 한국어 BERT Language model\n",
    "    - ETRI에서 KoBERT 공개\n",
    "    - 학습 데이터 : 신문기사 및 백과사전 등 23GB의 대용량 텍스트에 대해 47억개의 형태소를 사용하여 학습\n",
    "    - 형태소 분석 기반의 언어모델과 형태소 분석을 하지 않은 어절기반의 언어 모델있음 (당연히 전자가 더 좋음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 참고문헌\n",
    "- https://cs.kangwon.ac.kr/~leeck/NLP/01_intro.pdf\n",
    "- 인공지능과 자연어 처리 기술 동향.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
